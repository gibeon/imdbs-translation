## 摘要
本文概述了内存数据库系统的最新发展动态。随着内存容量的增加以及内存价格以每五年十倍的速度下降，内存中拥有"宿主"（primary home）的数据已经成为现实。内存数据库避开了许多针对磁盘驻留数据而优化的关系数据库系统的传统架构支柱。这些内存优化设计的结果是，系统具有了几种解决基本问题的创新方法（例如，并发控制、查询处理），与传统设计相比这些方法可以实现数量级的性能改进。我们的调查涵盖了构建高性能内存优化数据库时需要解决的五个主要问题和做出的选择：数据组织和存储、索引、并发控制、持久性、恢复技术以及查询处理与编译。我们将调查重点发在四个商业研究系统上：H-Store/VoltDB、Hekaton、HyPer和SAP HANA.这些系统的设计选择各不相同，构成了内存数据库系统中最先进技术的代表性样本。当然我们还涵盖了其他的商业学术系统，以及当前和未来的研究趋势。
## 1. 引言
&emsp;&emsp;内存数据库的研究与开发始于八十年代初期[37]，一些商业化的系统出现在九十年代，例如：TimesTen [146], P\*Time [28], DataBlitz [16]。其中的许多系统用于有针对性的、以性能为关键的应用中，主要是电信和金融领域。在这段时间内，内存的价格和容量很大的限制了这些引擎的适用性，因此在当时，内储系统没有作为通用数据处理解决方案。  
&emsp;&emsp;最近，有两种趋势使得这个领域再次火热起来：内存价格和多核并行性。在过去的30年里，内存的价格每5年下降10倍。现在，具有32个内核和1TB内存的服务器的成本约4万美元。诸如此类的机器使得以合理的价格将世界上大多数（不是全部）OLTP工作负载合适地安装到存储器中是可行的<sup>1</sup>(<sup>1</sup>本次调查的重点主要是内存数据库)。除此之外，现代CPU提供了惊人数量的原始并行性。普通CPU至少包含8个内核，现代服务器通常包含2到4个CPU插槽（16到32个内核）。内核的数量持续增加，英特尔目前正在发售具有18核的Xeon CPU [1]。  
&emsp;&emsp;这种并行性与将数据完全（实际地）存储在存储器中的能力相结合，引发了最近对内存数据库研究和开发的热潮。研究与开发的结果是使人震惊的。诸如H-Store [142] 和HyPeR [72]等著名的研究系统重新研究了内存和多核数据处理技术。大多数内存数据库供应商现在都有内存数据库的解决方案，例如SAP HANA [137]，Oracle TimesTen [74]和 Microsoft SQL Server Hekaton [38]。此外，许多初创公司如VoltDB [143]和MemSQL [2]已经在数据库供应商的领域中占据了一席之地。   
&emsp;&emsp;研究和开发的结果是产生了新一代的数据库系统，与传统的基于磁盘的相关系统相比此系统在设计上与其完全不同。这些系统放弃了许多“教科书”式的设计原则，转而支持新的（或者重新审视）的方法来实现现代软件的高性能。例如，下面的例子给出了这些系统的不同之处：  
* **数据组织与索引.**
在所有系统中，普遍的趋势是避免通过缓冲池进行基于页面的间接寻址，并且只将记录存储在内存中。索引通常存储直接指向记录的指针。一些系统还实现了新颖的方法，可以优化CPU的缓存效率[85]以及无锁存设计的多核并行性[89]。
* **并发控制.**  由于阻塞和上下文切换的开销，大多数系统避免了基于锁的悲观并发控制。相反，有一些系统使用多版本并发控制的变体[11,77,74,111]，而其他系统则使用分区串行执行来实现高性能[72,142,143]。
* **持久性和恢复.** Aries风格的重做/撤消（redo/undo）日志记录和恢复是很少被使用的。大多数系统选择一种仅重做日志（或命令日志）以及定期数据库快照来从崩溃或重新启动中恢复[38,74,142]。
* **查询处理和编译.** 为了避免虚函数调用的开销，分支预测的降级以及字节解释，一些系统放弃了"get next"的迭代器处理模型。相反，查询被编译为高度优化的机器代码，并直接在内存中记录[38,47,109].        
&emsp;&emsp;&emsp;![](/images/pic.png.png)   
图1.1:Microsoft Hekaton vs传统的SQL Server引擎，取自[38]。锁存器争用在很大程度上限制了传统引擎的可伸缩性。   


上面的表类似于曾经被认为在数据库研究文献中被挑选和"closed"的集合主题。然而，在过去几年中，随着主存数据库系统的重新出现，这些和其他核心领域的重大创新，使其成为一个充满活力和令人兴奋的技术空间。   
&emsp;&emsp;主存数据库的创新给性能带来了很有意义的提升。图1.1提供了与运行典型客户工作负载的传统SQL Server引擎相比，Microsoft Hekaton引擎的线程级可伸缩性的评估<sup>2</sup>（<sup>2</sup>来自原稿[38]的图）。Hekaton在12个内核上实现了大约15.7倍的性能改进，而传统引擎的可伸缩性受到运行内存绑定工作负载的、基于磁盘的、体系结构固有的开销的限制。   
&emsp;&emsp;本文综述了现代内存数据库系统的研究与开发.我们主要从四个主存系统的角度来考察事务处理引擎:(1)H-Store/VoltDB[142，143]，这是一个来自麻省理工学院、布朗大学、布兰代斯大学和耶鲁大学的学术研究小组的开创性研究系统，后来成为一个商业数据库；（2）Hekaton[38]，Microsoft SQL Server的主存OLTP引擎； （3）Hyper[72]，来自TU Munich的一个突出的研究系统，旨在支持在同一引擎中的高性能OLTP和OLAP工作负载；（4）SAP HANA[137]，第一个从主要数据库供应商提供的主内存优化引擎。正如我们将看到的，这些系统中的每一个在它们的设计选择上都是不同的，并且形成了内存数据库系统中现有技术的代表性示例。我们的讨论集中于影响系统架构和设计的五个问题: (a)数据存储和布局，(b)索引和数据结构设计，(c)并发控制，(d)持久性和恢复、(e)查询处理和编译。我们还总结了其他现代商业和学术主存系统的设计。    
&emsp;&emsp;这次研究的组织如下：第二章总结了“现代（modern era）”之前（大约从2007年开始）主存数据库系统的研究和开发；第三章总结了我们在系统调研中所使用的问题；第四章给出了四个具有代表性的系统的设计实例；第五章总结了主存数据库的研究现状和发展趋。    
## 2. 发展历史与趋势     
&emsp;&emsp;虽然本文主要关注现代内存数据库系统的体系结构和实现技术，但对内存数据库的研究却可以追溯到20世纪80年代中期。本节介绍了内存库研究的历史。此外，在内存系统中有一个超出数据库的更广泛的趋势。本节还重点介绍了数据库系统空间之外的几个主存存优化领域。
### 2.1 发展历史     
&emsp;&emsp;虽然目前在内存数据库领域有大量的活动，但实际上早在八十年代中期，内存优化系统的研究和开发就已经有了很长的历史。本节总结了这一早期工作，并将其与当今系统所采用的体系结构进行了比较。   
#### 2.1.1 早期发展（1984-1994）   
&emsp;&emsp;80年代初，关于内存数据库技术的研究论文开始出现。通常，大部分工作都着眼于提高传统关系数据库系统的性能并且假设大部分或所有数据库都适合于内存。    
&emsp;&emsp;1976年发布的IMS Fast Path[50]是最早针对内存驻留数据进行优化的已知系统之一。Fast Path要求用户将数据库指定为内存驻留数据库。优化包括细粒度记录级锁定和在提交时安装记录更新。Wisconsin大学的早期工作[37]探索了在大多数或所有关系都适合主存的情况下优化关系数据库系统。这项工作探讨了许多领域，包括访问方法性能、连接技术和恢复优化，如快速提交、组提交、并行日志I/O和检查点技术。    
&emsp;&emsp;Wisconsin大学的MM-DBMS项目[80，79，81]在内存数据库的各个领域进行了开创性的研究。MM-DBMS探索了使用指针进行直接记录访问、新的内存优化索引方法（T-Trees）、优化的两阶段锁定和新的恢复技术。IBM Starburst内存驻留存储引擎[82]基于MM-DBMS中提出的几种优化（例如，T-Trees、恢复技术）后续工作[52]探讨了StarBurst中的锁和闩锁优化，该优化建议使用单个闩锁来保护表、其所有索引和相关的锁信息。这项工作还提出了锁数据的直接寻址，以避免间接寻址。   
&emsp;&emsp;MARS系统[41,58]将系统划分为内存数据库处理器（DP）和恢复处理器（RP），其中RP懒惰地将易失性更新复制到稳定存储。MARS中的恢复工作显示了避免在主存储器系统中进行撤消日志记录的好处。系统M[134，135]是为研究主存环境中的恢复技术而构建的原型。它还探索了一种分区技术，该技术使用消息服务器来处理事务，并使用日志服务器来持久化更新。        
&emsp;&emsp;TPK [90]探索了多处理器主存储器环境中的事务处理。TPK以串行方式执行事务，并使用一组专用线程（输入，输出，执行和恢复）来处理事务。   
&emsp;&emsp;IBM[21，156]的Office by-example(OBE)探讨了集成Office系统中主内存数据查询处理的性能和优化。OBE研究了几种主要针对读数据的主内存优化，包括依赖指针进行直接内存访问的排序反向索引。    
&emsp;&emsp;PRISMA/DB[12，13]研究了分区DBMS上的两阶段锁和两阶段提交，其中每个节点都将数据存储在主存中。   
#### 2.1.2 发展的新纪元（1994-2007）   
&emsp;&emsp;内存数据库系统的研究始于90年代中期，反映了当今主存系统的发展趋势和体系结构。例如，像ClustRa [66]这样的系统探索了分布式主存储器架构，这是一个设计租户，其次是VoltDB和其他几个研究原型。在这个时代建造的一些原型系统已经成熟，成为了今天的高性能内存引擎。例如，P\*Time是SAP HANA中的主内存事务处理引擎，而TimesTen则被Oracle收购。    
&emsp;&emsp;Dalí是贝尔实验室从九十年代初开始建造的主存数据库引擎[67,23]。Dalí后来被商业化为DataBlitz [16]。该系统针对OLTP工作负载的高性能多线程执行，为AT＆T的许多内部高性能应用程序提供服务。Dalí允许应用程序直接共享内存来访问数据记录，这种技术允许非常快速的执行，但在现代内存系统中并不常见。DataBlitz/Dalí支持数据复制以实现快速故障转移。它还通过执行仅重做日志来减少恢复期间的I/O（暂时撤消日志在内存中维护并在提交后丢弃）。重做日志记录是当今内存系统中的常见做法。DataBlitz/Dalí还采用模糊操作一致的检查点，减少了检查点工作者的锁争用。    
&emsp;&emsp;几个分布式内存架构开始出现在九十年代中后期。ClustRa数据库[66]是为电信工作负载定制的分布式内存系统 ClustRa专门为高性能和可用性而构建，并在每个节点上完全复制数据库状态。它使用具有独立故障模式的2-safe复制方案。    
&emsp;&emsp;来自纽约大学的系统K [157]探索了一种用于高性能内存中事务处理的编程语言方法。该系统在分配给CPU核心的内存分区上顺序处理事务（它没有分布在多台计算机上）。系统k使用逻辑日志记录，这在当今的几个主内存数据库（如Hekaton和Voltdb）中很常见，并且用户还提供了指定冲突事务类型的提示。    
&emsp;&emsp;TimesTen主内存数据库[146,147,148]最初是惠普实验室的一个为“Smallbase”的研究项目，后来被分拆成一家独立的公司。与其他主内存系统一样，TimeSten实现了一个检查点和日志方案，该方案针对主内存执行进行了优化。TimesTen的一个独特之处在于它既可以作为服务器使用，也可以作为可以通过应用程序直接链接的库使用。TimesTen还支持完全ACID事务，但允许用户放松ACID属性以获得更高的性能。这种灵活性允许TimesTen引擎用于其他应用场景，例如中间层缓存[148]。TimesTen被Oracle收购，现在作为Oracle的高性能主内存数据库的解决方案。  
&emsp;&emsp;P\*Time [28]是一个轻量级的主内存OLTP数据库，专为商用机器的可扩展性而构建。该系统针对二级缓存（L2-cache）的观念进行了优化。P\*Time中的几个内部数据结构完全无锁存：这种趋势在其他现代主存储系统中很常见，例如Hekaton。P\*ime还通过执行细粒度并行差分日志和恢复来优化持久性和恢复。并发控制涉及多级锁。虽然P\*Time最初是作为研究原型开发的，但它现已成为SAP HANA数据库中的内存OLTP引擎。    
### 2.2 发展趋势   
&emsp;&emsp;目前在更广泛的系统和存储空间的研究中都假设存在内存驻留数据。虽然本次调查的重点是内存数据库，但我们也重点介绍了内存存储的两个相关趋势。    
#### 嵌入式键值存储     
&emsp;&emsp;内存键值存储是在单个记录上提供原子操作（例如，更新，删除）的引擎。非分布式键值存储通常可用作嵌入较大应用程序或系统的库，以提供高性能记录存储和访问。嵌入式键值存储的当前趋势是提供高性能，内存优化的数据结构（基于散列或基于范围的访问），以实现快速数据操作和检索。如果需要持久性，则键值存储通过写入辅助存储（通常是闪存）或通过复制来使更新持久。     
&emsp;&emsp;内存优化的键值存储设计的示例包括Masstree [97]，Bw-tree [89]和MICA [92]。Masstree是一个内存优化的键值存储，可在现代对称多处理机器上提供范围查询和缩放。Masstree使用“trie-of-B+-trees”采用具有缓存观念的索引布局，并实现了一种不会阻塞读者的乐观并发控制方案。Bw树在内存中是一个完全无锁存的B+-tree。为了持久性，它实现了一个日志结构的页面存储，它按顺序写回稳定的存储（例如，Flash SSD）。Bw树的内存部分也作为Hekaton的范围索引。MICA是一个非常高性能的键值存储，可以优化整个请求处理堆栈，从网络接口卡上的网络请求处理到数据结构设计。MICA使用有损哈希索引，该索引可以很好地扩展并发执行读取和写入的并发线程访问。    
#### 分布式键值存储    
&emsp;&emsp;横向扩展分布式内存中键值存储是通过降低商品网络基础设施（例如RDMA）中的内存价格和价格/性能改善而实现的另一个趋势。也许在实践中分布式键值存储的最常见用途是缓存。Memcached[99]是应用最广泛的缓存系统之一。虽然它是通用的，但在实践中它被用作Web应用程序的前端缓存。例如，据报道，Facebook在2009年使用了超过2,000台memcached服务器来缓存大约25％的数据[117]。     
&emsp;&emsp;最近的项目为主要的数据存储构建了分布式内存键值存储。与缓存不同，这些系统在确保了持久性的同时也保持了高性能。RAM-Cloud[116]是第一个探索这种方法的项目，其目标是将基于磁盘的分布式存储系统的性能提高100倍至1000倍，延迟为5µs至10µs。RAMCloud将一组存储服务器组织为分布式横向扩展内存池，用于记录存储。为了实现其性能目，RAMCloud项目在恢复[114]和内存分配[133]等基本领域取得了一些进展。Farm[39]是另一个向外扩展的分布式内存键值存储。Farm具有比RamCloud更低的级别接口，将来自计算机集群的内存作为共享地址空间而公开。Farm的功能包括通过RDMA进行无锁读取，以及在单机上本地运行事务的函数交付。
## 3. 问题和体系结构选择  
&emsp; 内存数据库系统的体系结构和实现技术不同于传统的关系数据库。在本节中，我们将讨论一些允许内存系统实现高性能的问题。这些问题中的每一个都会影响体系结构的选择，并最终决定系统的设计。对于每一个问题，我们首先总结了传统的基于磁盘的系统是如何处理它的。 然后，我们描述了内存环境的不同之处，并总结了现代系统可用的各种设计方案。在后面的第4章中，我们将详细介绍我们的代表性系统如何在实践中解决这些问题。  
### 3.1 数据组织和布局  
&emsp; 本节介绍内存数据库系统中的基本组织和布局问题。正如我们将要看到的，主要的设计原则是避免基于磁盘的关系系统中基于页面的间接寻址。此外，我们总结了现代内存系统做出的一些组织决策，例如分区，多版本控制和行/列布局。  

&emsp; &emsp; &emsp; ![](/images/Figure3_1.png)  
#### 3.1.1 基于磁盘的关系系统组织  
&emsp; 传统的关系数据库系统是在假设并非所有数据都适合内存的情况下构建的，这在关系系统的早期确实是一种宝贵的资源。在这些系统中，数据的“最终归宿”是磁盘。这意味着，在处理过程中，必须根据需要将数据分页到内存中。  
&emsp; 数据库页面是固定大小的块，通常大小为几千字节（例如，典型值为8KB或更大），并且在磁盘和内存中具有相同的单位，以避免单位之间的转换开销。系统中的几乎所有数据都映射到页面。 例如，数据库记录映射到页面上的特定偏移量和长度，而像B+树这样的索引数据结构使用页面大小节点。  
&emsp; 从磁盘分页数据由数据库存储引擎处理。在内存中，页面存储在共享缓冲池中，如图3.1所示。缓冲池是一个页框数组，其中一个框是数据库页面的大小。虽然实现细节不同，但使用哈希表将页面标识符映射到缓冲池中页面的物理位置，以及磁盘位置和其他元数据等其他信息（有关缓冲池实现的更多详细信息，请参阅[63]）。当存储引擎收到页面请求时，它首先在哈希表中执行查找以确定页面是否驻留在缓冲池中。如果页面不在内存中，则存储引擎会发出I/O请求，以将页面从磁盘带入缓冲池。然后返回指向内存页面框的指针。缓冲池是一种优雅而简单的分页解决方案，它将磁盘从系统中的其他软件组件（例如查询处理器）中抽象出来。但是，正如我们将在下一节中看到的，缓冲池间接操作严重影响内存数据库系统的性能。  
#### 3.1.2 内存储系统中的数据组织  
&emsp; 内存数据库中记录的主要宿主是RAM。这些系统不受在查询处理过程中按需在磁盘上来回分页数据的需求的限制。正是由于这个原因，现代内存数据库通过缓冲池避免了基于页面的间接寻址。这些系统不使用传统数据库系统中所做形式（页面ID，偏移）的逻辑记录标识符。相反，内存通常使用内存中的指针来直接访问记录。  
##### 避免页面间接寻址的性能优势  
&emsp; 避免缓冲池间接可以将数据库引擎的性能提高一个数量级。如此大的提高主要有两个原因:（1）避免间接解析物理记录指针；（2）缓冲池中的页级锁存争用。下面我们将详细介绍这两种开销。  
&emsp; **避免基于页面的间接寻址**。通过缓冲池访问记录会导致主存储器系统中不必要的开销。为了理解这一点，请注意图3.1中对内存缓冲池页面的访问需要两层间接方式来解析物理记录指针：（1）通过缓冲池中的页面的目录哈希表访问页面；（2）使用页面内存块中的偏移量计算指向记录内存的指针。在每次访问记录时避免这种间接访问是一个巨大的性能优势。来自Starburst内存引擎项目的实验评估为这种加速提供了实验依据[82]。这些实验表明，与使用分页缓冲池间接寻址相比，在StarBurst引擎中使用物理记录指针时，性能提高了一个数量级。  
&emsp; **避免页面锁定**。避免缓冲池间接访问移除与页级锁存相关的开销。例如，通过缓冲池访问页需要访问者在读取时设置共享页锁存器，或者在更新时设置独占页锁存器。操作这些页级锁存器可能成为瓶颈。例如，更新将设置一个锁存变量来标记它有独占访问权限来操作页面内存。甚至使用共享锁存器的读取器也将操作锁存变量（例如，引用计数）以指示并发读取器的数量。锁存变量的更新是使用原子cpu指令完成的，这可能导致性能下降，尤其是在多套接字计算机上（商业系统试图使用分区锁存器（有时称为超级锁存器）来解决这一瓶颈，尽管这种方法很有用，但并不能避免所有争用。）。在商业系统中，基于访问者的行为具有多个锁存器类型并不少见。 例如，Microsoft SQL Server有六种锁存类型[35]，这意味着一个线程可以为单个页面访问获取多个锁存器。  
&emsp; **端到端示例**。为了说明页面间接寻址和锁存开销，请考虑在基于磁盘的系统中进行辅助B+树遍历。此操作首先需要遍历需要在缓冲池中进行四次页面查找的辅助索引，以及四次锁存/解锁操作和四次二进制搜索。如果辅助索引不包含所有必需的列，则可能需要遍历主索引，（在SQL Server这样将主键存储在辅助索引中的系统中，这是必需的。）从而使操作成本增加一倍。此外，由于事务锁定（取决于隔离级别和并发控制方案），可能会有额外的开销。总的来说，与存储直接指向记录的指针的内存系统（如Hekaton）相比，这种辅助索引遍历的开销要大得多。在Hekaton中，辅助索引查找只需遍历索引执行四次索引节点搜索，完全避免了缓冲池和锁存开销。  
##### 历史概述  
&emsp; University of Wisconsin在内存数据库方面的一些最早的研究着眼于优化数据库操作，假设数据库能够很好地适应缓冲池[37]。这项工作揭示了内存驻留数据的许多性能改进，但是，即使数据库页被很好地缓存在内存中，记录访问仍然需要如上所述的缓冲池间接访问。University of Wisconsin的MM-DBMS项目[80、79、81]以及后来的IBM Starburst的项目[82]是最先探索在数据库引擎中使用指针进行直接记录访问的项目之一。从这一点上说，大多数内存系统也避免了缓冲池的间接使用。PRISMA/DB[12，13]、TIMESTEN（及其研究前身“SmallBase”）[146，147，148]和P\*Time[28]等系统都使用直接记录指针。一个值得注意的系统是贝尔实验室的Dali，它走到了极端，允许应用程序直接共享内存访问数据记录。虽然这种技术导致非常快的执行速度，但它在现代内存系统中并不常见。   
##### 现代内存系统的组织选择  
&emsp; 虽然直接指针访问记录在现代内存系统中很常见，但它们在记录如何在内存中的组织的方式上有所不同。本节总结了三种主要的组织选择。在后面的第4章中，我们将介绍我们的每一个现代代表性的系统如何选择组织数据。  
&emsp; **分区**。现代内存系统之间最大的区别之一是它们是否对数据库进行物理分区。H-Store和VoltDB是分区系统的示例，而Hekaton，HANA，MemSQL和Oracle TimesTen是非分区系统。  
&emsp; 分区的优点是它极大地简化了线程和事务级并发问题。例如，分区系统（如H-Store）在分区内串行运行事务，从而避免了用于单个分区事务的事务并发控制协议。此外，分区系统通常在分区内单线程运行，例如，通过将分区分配给单个CPU核心。单线程访问意味着内部数据结构（如索引）不需要支持多线程更新，这可以大大简化数据库内核的实现。  
&emsp; 非分区方法的优点是任何线程都可以访问和更新数据库中的任何记录。因此，这些系统避免了由于“热”分区而可能出现的负载平衡问题，这通常需要在线程之间重新分配工作负载，并且可能需要数据移动（例如，跨NUMA套接字）。此外，非分区系统不需要协调跨分区的事务，因为默认情况下，系统允许对所有线程共享数据访问。另一方面，非分区系统中引擎实现的复杂度高于分区系统；实际上，非分区系统牺牲了引擎实现的复杂性来换取分区管理的复杂性。  
&emsp; **多版本**。第二个重要的体系结构选择是系统是否支持数据的多版本化。虽然多版本化在数据库中不是一个新概念，但它在内存系统的范围内重新引起了人们的兴趣。主要原因是它使能了读取器从不阻塞写入器的并发控制方案。与导致线程在冲突时休眠的阻塞的协议相比，这种非阻塞行为导致更少的上下文切换开销（例如，在锁管理器中）。由于上下文切换是主存储器系统中的大开销，因此这样的协议可以导致显著的性能改进。我们稍后将在第3.3节中讨论并发控制的详细信息。例如，出于这个原因，Hekaton、Hyper和SAP HANA实现了记录多版本化。多版本的另一个用途是允许读取过去的快照，以便支持对“接近”实时数据的分析查询。Hyper和SAP HANA都支持这类功能。  
&emsp; **行/列布局**。第三种组织选择是以行格式还是列格式存储记录。数据库文献[4]中对行与列布局的权衡进行了深入研究，行存储用于更新密集型OLTP工作负载，而列存储更适合于分析工作负载。对于内存系统，这个经验法则没有太大变化。由于本调查主要关注的是可操作的OLTP系统，因此我们所涉及的大多数系统都选择行布局，以便进行有效的更新。然而，正如我们将在第四章后面看到的，有一些明显的例外。例如，SAP HANA支持允许OLTP工作负载直接对列存储中的数据进行操作的执行模式。在这种模式下，HANA愿意牺牲一些OLTP性能（例如，与纯行存储相比），以获得以列格式存储主数据库的优势。HyPer支持混合存储格式，可以对频繁访问的列进行聚类。Hekaton以行格式存储表，但可以在表上创建列存储索引，以加快分析查询。  
### 3.2 索引  
&emsp; 现代内存系统使人们对高性能索引方法产生了新的兴趣。在本节中，我们首先总结了传统数据库系统中的索引设计。然后探讨了索引完全适合主存时的两个重要问题:（1）优化CPU缓存效率；（2）多核并行性。在本节中，我们使用B+树作为典型索引的示例，因为所有主要的商业关系数据库系统都支持它们。   
#### 3.2.1 基于磁盘的关系系统中的索引  
&emsp; 在典型的基于磁盘的系统中，索引映射到由缓冲池管理的页。这允许索引分页到磁盘和从磁盘分页，类似于包含记录的页。图3.2描述了关系数据库中B+树的组织。虽然特定的节点布局在系统之间可能不同（例如，键和其他元数据），但索引节点具有页面的确切大小。这意味着节点可能未得到充分利用（即，部分页面可能为空）。此外，当页面变满时，节点必须拆分。  
&emsp; &emsp; &emsp; ![](/images/Figure3_2.png)

&emsp; 基于磁盘的系统通常还支持按索引定义的顺序存储记录的聚集索引。群集确保记录在磁盘上保持排序顺序，通过减少随机磁盘查找，使范围扫描之类的操作更加高效。每个表只能聚集一个索引，其余索引必须是辅助索引。商业系统在如何实现辅助索引方面有所不同。例如，Microsoft SQL Server将行的主键存储在其叶页中，需要在主索引中进行第二次查找以检索完整记录。 Oracle将主键和直接记录指针都存储在其叶节点中，将直接指针用作查找记录的“快速路径”，并在记录移动时将主键用作慢速路径[63]。
#### 3.2.2 内存系统中的索引  
&emsp; 在内存数据库中，索引的组织方式与基于磁盘的系统有所不同。首先，不存在第3.1节中讨论的基于页面的间接寻址，因此索引节点不需要映射到数据库页面。例如，Hyper中使用的ART基数索引为其索引节点使用四个不同的大小类。同样，Hekaton中使用的Bw树索引准确地分配了存储其键和有效负载所需的节点内存；它不会在节点中留下未使用的空间。此外，Bw树使用灵活的拆分策略，选择在方便时拆分B+树节点，而不是像基于磁盘的索引那样在节点大小达到硬阈值时才拆分B+树节点。  
&emsp; 内存索引也通常存储指向记录的直接指针，而不是像在基于磁盘的系统中那样存储逻辑记录ID或主键（在辅助索引的情况下）。图3.3提供了这种差异的可视化示例。图3.3(a)描述了一个基于磁盘的聚集索引，其中索引指向按排序顺序存储记录的数据（叶）页。另一方面，图3.3(b)描述了内存方法，该方法对内存中的记录进行索引，而不进行任何特定的集群或组织。  

&emsp; &emsp; &emsp; ![](/images/Figure3_3.png)
##### Cache Awareness  
&emsp; 在研究和实践中，缓存感知索引结构一直是人们感兴趣的主题。原因是CPU和内存速度之间的差距。从上世纪90年代开始，当时的CPU速度每年增长60%，而内存速度每年仅增长10%。不足为奇的是，缓存未命中停止开始成为内存绑定数据库工作负载的主要瓶颈，很大一部分时间用于处理末级缓存未命中[8，113]。今天，CPU/内存的性能差距仍然很大。  
&emsp; 为了解决这一瓶颈，索引研究集中在如何优化使用内存系统中的“传输单元”--缓存行。CSB+树[127]（如图3.4所示）等技术使用缓存行大小的B+树节点，这些节点删除了尽可能多的搜索指针，以便为键腾出空间。为了处理搜索指针的删除，CSB+树使用“节点组”将多个节点打包在同一个内存块中。父节点存储一个或多个指向子节点组的指针（指针/节点组的数量是可配置的），并且通过使用节点组中的偏移量来访问子节点。

&emsp; &emsp; &emsp; ![](/images/Figure3_4.png)  

&emsp; 缓存行预取在内存索引的上下文中也被探讨了。PB+树[30]研究了B+树节点的预取，以便使搜索和扫描更有效，并允许节点大小大于缓存行。当时，这项技术显示B+树搜索的速度提高了1.5倍。扫描性能提高了6倍，但是它需要一个额外的结构来存储指向扫描中未来节点的指针。  
##### 多核并行  
&emsp;  虽然缓存高效索引仍然是当今的一个重要问题，但多核可伸缩性也变得同样重要。如今，主内存数据库以惊人的并行度运行在CPU上，每一代处理器都会增加更多的内核数量。例如，今天，在跨越多个套接字的服务器上通常有16到32个处理器内核，而高端计算机最多有144个内核（联想X3950可以配置12TB RAM和144个内核）。 因为索引是更新和检索的热门路径，因此注意并行性对于OLTP系统尤其重要。  
&emsp;  通常，在内存索引结构中有两种方法可以实现高并发性。  
&emsp; **分区**。这种方法对索引结构进行分区，并为每个分区分配一个线程。每个线程都有对其分区的独占访问权。由于线程之间互不干扰，因此索引实现仍然相对简单，线程级并发性不是问题。像PLP[119]这样使用多根B+树（分区由每个根定义）和H-Store技术就是这种方法的代表。虽然分区方法在概念上易于实现，但该方法的缺点是工作负载中的偏斜可能导致某些分区成为热点，从而限制系统的吞吐量。因此需要重新平衡以移除热点。  
&emsp; **共享**。共享方法允许任何线程读取或更新索引的任何部分。高性能共享方法的困难在于维护索引的高并发性和可伸缩性。实现高并发性的关键是通过阻塞其他线程来避免（尽可能避免）提供互斥的关键部分。为了解决这个问题，最近研究了“无锁存”（lock-free）索引结构，该结构使用原子CPU原语（例如，compare and swap，fetch and increment）来更新索引状态。共享索引方法的优点是它们是自平衡的:任何线程都可以访问索引的任何部分。缺点是实现复杂，尤其是对于不能依赖于关键部分的无锁存方法。Hekaton中使用的Bw树和MemSQL使用的skiplists是实践中使用的高度并发的无锁存索引的示例。Mass-Tree[97]也是这方面最先进工作的代表。我们将在后面的章节中详细介绍这些方法。  
&emsp; 虽然在实践中使用了几种无锁存索引实现，但最近的研究已经开始研究无锁存同步的优缺点。Failero和Abadi分析了几种锁存和无锁存同步技术，并执行了微基准测试，以实验验证每种技术的相对优势[44]。他们发现无锁存同步的好处微乎其微。无锁存技术带来高性能，但需要详细的内存回收协议。研究还发现，无论同步方法是基于锁存还是无锁存，现代CPU可伸缩性的最重要因素之一是避免全局存储器位置上的争用。  
### 3.3 并发控制  
&emsp; 并发控制是数据库研究中的一个长期话题，可以追溯到关系数据库的早期[55]。已有一些关于这个问题的书籍[20、57、155]，以及一些全面的调查[18、19]。然而，内存数据库已经引起了对并发控制领域的重新审视，其目标是在现代硬件上实现高性能。  
&emsp; 在较高的层次上，并发控制设计空间有两个维度：  
* 乐观法vs悲观法：悲观的方法在事务执行期间检测并发冲突，并中止事务（导致重新启动）或阻止事务执行，直到冲突清除。乐观方法允许事务在不阻塞的情况下执行。在提交过程中，这些方法需要一个验证步骤来检测（和中止/回滚）具有并发冲突的事务。  
* 单版本与多版本：单版本方法只维护记录的一个版本。记录在适当的位置更新，并且在任何时间点只有一个版本可供读取器使用。因此，读取器可能会阻止写入器。多版本方法在每次更新时创建新记录版本，并允许读取器读取旧版本，从而避免阻塞写入器。  

&emsp; 许多早期的研究探讨了不同并发控制技术的权衡。这些研究中有许多导致了相互矛盾的结果。例如，一些比较表明，基于悲观阻塞的方法的性能优于重新启动交易的悲观方法[7，25]，而另一些研究则相反[145]。在其他研究中，乐观并发方案的性能优于悲观的基于锁定的方法[46]，而在其他比较中则相反[7，25]。Agrawal等人[6]所做的一个更全面的比较澄清了由于对计算资源的假设而产生的许多相互冲突的结果，并建议乐观的方法在冲突率较低时表现得非常好，但在冲突率较高时（由于回滚和重新启动）性能会下降。  
&emsp; 总的来说，这些早期研究的许多结果仍然成立，例如关于争论对乐观方法的影响的基本发现。然而，由于现代主存环境中并行性的提高和磁盘开销的缺乏，许多系统体系结构重新审视了并发控制设计空间。在本节的其余部分中，我们首先回顾基于磁盘的系统中常用的两阶段封锁协议。然后，我们再对内存系统中使用的技术进行了概述。正如我们将看到，现代系统中的一个共同主题是通过使用分区串行执行来利用并行性，或者使用某种形式的乐观多版本化。然而，最近的其他工作表明，在某些情况下，单一版本的悲观方法也能很好地运作。   
#### 3.3.1 基于磁盘系统中的并发控制  
&emsp; 商业的基于磁盘的关系数据库都使用两阶段锁定（2PL）的形式。 2PL是悲观的协议，要求所有读取器在读取记录之前获得共享读取锁，在写入记录时获得独占锁。可以对较大粒度的项（例如，页上的一组记录或整个表）使用锁，以减少获取多个记录上的单个锁的开销。如果事务无法获取锁，则它将阻塞，直到授予锁。在事务执行时，它获取并持有锁，直到事务结束（第一阶段），并在终止时一次性释放所有锁（第二阶段）。  
&emsp; 大多数基于磁盘的商业系统也支持多版本化，并且在一定程度上支持乐观的方法。这两者通常都被支持作为2PL[63]的附加组件。例如，SQL Server使用多版本控制来支持快照隔离，但版本是临时的，不可恢复。在本节的其余部分中，我们只关注2PL协议。  
&emsp; 关系引擎中的锁管理器负责实现2PL协议。请求锁时，事务向锁管理器提供其事务ID、资源ID（要锁定的项的唯一标识符）和请求的锁模式（有关详细信息，请参见[57])。锁管理器要么授权请求，允许事务继续，要么阻塞事务，一旦资源可用，就重新启动事务（大多数锁管理器还提供非阻塞请求来检查锁是否可用）。
&emsp; 虽然锁管理器实现因系统而异，但通常使用两个主要数据结构来处理锁定簿记：（1）锁表是用于管理资源和检测锁冲突的结构。它通常实现为一个哈希表，该哈希表在资源ID上键入包含锁元数据的条目，如模式和阻塞事务的等待队列。（2）事务表保存有关在DBMS中执行的事务的元数据。该表存储诸如线程状态（例如，用于在事务被阻塞时恢复事务）和事务持有的所有锁的列表等信息，以便在事务终止时释放其所有锁。虽然我们没有提供2PL实现的详细信息（更多详细信息请参见[63])，但高层次的想法是所有主要的基于磁盘的系统都使用锁管理器进行并发控制。  
#### 3.3.2 内存系统中的并发控制   
&emsp; 内存数据库的一个共同趋势是避免实现单独的锁管理器。这样做是出于性能原因。锁管理器是一个大型的单片软件子系统，充当并发控制的“交通警察”。在访问记录之前，事务必须至少通过锁管理器的锁表进行一次往返。由于系统中其他地方存在的大量开销（例如，I / O），这种间接在基于磁盘的系统中是可以容忍的。然而，在主存储器系统中，这种间接方式是主要的性能瓶颈。因此，主存储器系统的一个主要趋势是在记录本身中嵌入并发控制元数据。第4章将介绍实践中几种方法的实现细节。在本节的其余部分中，我们将总结主存系统中并发控制的一般设计空间。  
&emsp; **多版本和乐观法**。在一些商业和研究系统中，一种常见的方法是实现多版本并发控制。像Hekaton、HANA和Hyper这样的系统都使用某种形式的多版本。采取这种方法的主要原因之一是解锁并发，因为读取器从不阻塞写入器。虽然并发性是一个优点，但多版本化的一个缺点是:(a)在更新期间创建新版本，(b)需要某种形式的垃圾收集来删除过时的版本。  
&emsp; 在多版本设计领域，系统倾向于使用乐观的方法，但也有例外。Hekaton和Hyper都使用一种乐观的多版本并发控制形式。使用乐观法的优点是它比传统的锁要便宜得多。它还可以扩展到大量的内核，因为线程只是无阻塞地执行，直到事务完成，避免了一种悲观的基于锁的方法中上下文切换等开销。如前所述，乐观法基本缺点是，高争用率可能导致高中止率，导致事务回滚并多次重新启动[6]。SAP HANA选择了一种悲观的多版本方法，使用行级写锁。  
&emsp; **分区**。如果数据库大小足够大，则选择分区的系统会按核心或计算机分配数据库的不相交部分。在分区中，事务按顺序执行。这种方法很简单:因为事务一个接一个地执行，所以既不需要锁，也不需要验证。这导致在单个分区中执行非常快，并在事务触及单个分区时带来很好的整体系统性能。但是，当一个事务跨越多个分区时，它必须在事务期间以独占方式锁定它访问的所有分区。 这可能导致性能降低和不可预测性。  
&emsp; H-Store/VoltDB和Calvin[150]是使用分区方法的示例系统。H-Store用一个独占锁保护每个分区。在访问分区之前，事务必须获取分区锁。如果事务发现它需要访问另一个分区（例如，由于辅助索引查找），则它将中止，并在获取所需分区上的所有锁后重新启动。Calvin是一个可以在分区非事务性存储系统（内存或基于磁盘的存储）上分层的系统。它实现了确定性锁定协议，消除了分布式提交协议中的典型开销。  
&emsp; **写/读集的先验知识**。如果系统可以在执行之前确定事务的读和写集合（例如，使用静态分析），则可以对并发控制协议进行多个优化。例如，Calvin中的确定性锁定协议依赖于读和写集的先验知识来确定序列化顺序。此外，许多用于内存系统的悲观并发控制技术依赖于先验写集知识。非常轻量级的日志记录[130]是一种基于锁的方法，它将锁元数据（例如，独占锁和共享锁的计数）直接存储在记录中。事务在事务执行开始之前请求其所有锁，并且仅当所有锁都已授予时才开始执行。此外，BOHM[43]是一个悲观的多版本并发控制协议，它在执行之前确定事务的序列化顺序。然后，BOHM在内存中为事务将要写入的新版本创建一个记录占位符。这种方法避免使用集中的时间戳生成器以及乐观的多版本方法中常见的验证阶段。  
### 3.4 持久性和恢复   
&emsp; 持久性和恢复技术确保系统在崩溃后可以恢复到一致的状态。到目前为止，基于磁盘的系统使用的主要崩溃恢复方法是ARIES[102]日志记录和检查点协议。一般而言，内存系统中的持久性和恢复仍然基于日志和检查点。然而，在检查细节时，基于磁盘的系统和内存系统之间的差别是巨大的。出于性能原因，许多内存系统避免使用教科书中的ARIES式协议。在本节的其余部分中，我们将简要介绍基于磁盘的系统所使用的Aries风格的方法。然后，我们突出显示内存系统不同的最值得注意的地方。第4章详细介绍了具体系统在实践中所做的工作。
#### 3.4.1 基于磁盘的系统的持久性和恢复  
&emsp; 基于磁盘的关系数据库都使用ARIES[102]协议的形式来确保已提交事务的持久性并恢复到一致状态。如前所述，基于磁盘的系统会更新缓冲池中的数据库页。ARIES协议使用预写日志记录(WAL)方案，在将页面写回磁盘之前，将每次页面修改写到顺序日志中。每个日志记录都有一个确定其在日志中位置的日志序列号(LSN)，只有较低的LSN之前的所有的记录也在磁盘上时，才能将日志记录刷新到磁盘上。在外部向用户确认事务的成功之前，会将提交记录写入日志并刷新，以确保事务的效果即使在失败时也会成功。
&emsp; 日志记录包含足够的信息，可以重做和撤消更新的影响。在事务中止的情况下，回滚更新需要页面上更新字节序列的前映像（或如何撤消页面更新的逻辑说明）。在恢复过程中，需要重做信息才能将更新应用于页面。  
&emsp; 为了提高性能，数据库尝试将日志记录保持在尽可能小的范围内，以增加持久存储的I/O吞吐量。DBMS中有多种事务日志记录方法[59，62，68，96]。在一个极端，存在物理日志记录，其中DBMS记录被事务[59，103]修改的数据库中的元素（例如，索引的元组、块或内部数据结构）的前后映像。
&emsp; 另一种方法称为逻辑日志（有时称为事件日志），其中DBMS只记录事务执行的高级操作（例如，更新）。与物理日志记录相比，逻辑日志记录减少了需要写入磁盘的数据量，因为它只需要记录操作是什么，而不是实际对数据库执行的操作。但是使用逻辑日志恢复数据库需要更长的时间，因为DBMS需要重新执行每个操作。通常使用物理或生理日志（例如，描述特定页面上的逻辑操作）来支持重做，并使用逻辑日志来描述更新的撤消。基于磁盘的系统还记录索引结构的更新，如B+树或堆结构修改。这允许在恢复期间快速恢复索引。
&emsp; 为了支持快速恢复，关系数据库通过将缓冲池中的页异步刷新回磁盘来定期检查数据库。这允许截断预写日志，并允许在历史记录点开始恢复，从而减少重做（和撤消）步骤的数量，使数据库恢复到一致的状态。恢复的细节超出了我们的研究范围（有关完整和彻底的讨论，请参阅ARIES [102]）。然而，出于比较的目的，我们注意到基于磁盘的系统通常使用ARIES协议和检查点的一些风格，将页面写回磁盘。  
#### 3.4.2 内存系统的持久性和恢复    
&emsp; 内存系统的日志记录是为高吞吐量和低延迟而优化的。由于日志I/O是主要的瓶颈，因此这些系统试图尽可能地减少日志卷，甚至比基于磁盘的系统减少得更多。例如，Hekaton通过只为保证提交的事务编写最新的记录版本来执行只重做日志记录[38]。这种方法完全避免了写入撤消信息。H-Store/VoltDB使用一种精简的逻辑日志记录变体，称为命令日志记录[96]，它只记录事务调用请求。此方案的日志记录仅由存储过程名称及其输入参数和事务ID组成。
&emsp; 为了进一步最小化日志通信量，一些系统还完全避免记录索引数据。只记录对基本数据的更新。崩溃之后，在恢复基本记录之后从头开始构建索引。此外，主存系统尽可能多地并行化日志I/O。例如，由于Hekaton使用多版本并发控制方案，因此它可以轻松地将日志流量并行化到多个I/O设备上。这也是筒仓回收原型[159]中采用的方法。出于性能原因，几乎所有系统都选择SSD作为日志设备。  
&emsp; 检查点在内存系统中也是不同的。内存检查点通常大于磁盘的系统中其对应的检查点；这是因为内存系统通常将所有（或大部分）行写入存储器。例如，Hekaton中的检查点方法不断扫描日志的尾部，并将新记录版本写入版本检查点文件。日志上的删除将在现有检查点文件上以增量形式写入，以表示文件中的“逻辑删除”记录。同时，也已经创建了其他轻量级主内存检查点技术，例如CALC [129]非阻塞异步方法，该方法进入临时写时复制模式，仅写入自上一个检查点以来发生的记录更新。H-Store/VoltDB还使用类似的写时复制方案来获取每个节点上数据的异步快照。
&emsp; 内存系统中的恢复过程不同于基于磁盘的系统使用的典型ARIES风格的“分析，重做，撤消”序列。大多数系统通过加载最后一个有效检查点来恢复，然后重播日志的尾部，从而避免完全撤消。例如，由于Hekaton只记录提交事务更新的重做信息，因此它从检查点文件中重新创建初始数据库。然后，它会在联机之前重播日志的相关部分。同样，H-Store / VoltDB通过加载最后一个完整一致的快照来恢复。然后，它从其命令日志（从上一个快照时间开始）重播事务，以使数据库处于一致状态。
&emsp; 在内存系统中，整个数据库必须在联机之前加载到内存中；数据库不能像基于磁盘的系统那样部分地存在于存储中。因此，恢复时间主要取决于使用检查点和日志重建数据库以及索引的时间。为了使这尽可能高效，很多系统尝试尽可能多地并行恢复。例如，Hekaton试图通过使用多个日志存储设备来最大化恢复期间的I/O带宽。 Silo使用值日志记录，它还实现了一个高度并行的日志重播方案，旨在避免不必要的分配和重播工作。
### 3.5 查询处理和编译
&emsp; 数据库系统的查询处理组件包含许多任务，从执行授权、解析和查询优化的“上层”到执行物理查询计划的“下层”。当将内存系统与基于磁盘的系统进行比较时，查询处理器上层的差别不大:解析和查询优化的实现仍然相似。查询处理堆栈的底层有很大的不同。内存系统通过将查询直接编译为机器代码来减少运行时开销。 这些查询直接针对内存中的数据执行。另一方面，基于磁盘的系统使用迭代器模型执行查询，并且经常使用运行时解释来执行谓词评估等操作。
&emsp; 本节总结查询执行堆栈较低层中基于磁盘的系统和内存系统之间的差异。我们首先讨论基于磁盘的系统如何实现迭代器模型。然后讨论了现代内存系统中的查询编译和执行策略。
#### 3.5.1 基于磁盘的系统中的查询执行  
&emsp; 在查询重写和优化之后，查询处理器负责执行物理查询计划。查询计划本质上是由各种查询操作符组成的数据流图。图3.5提供了一个由scan、join、sort和group by操作符组成的示例查询计划。
&emsp; &emsp; &emsp; ![](/images/Figure3_5.png)

&emsp; 查询使用迭代器模型执行（也称为Volcano样式处理[54])。每个运算符都实现一个泛型接口，该接口由以下基本函数组成：（1）初始化运算符的init，（2）指示运算符生成下一个元组并将其返回给调用方的get-next，以及（3）通知运算符取消初始化的close。每个操作符接受一个或多个迭代器作为输入（取决于操作的语义）。这个接口虽然简单，但功能强大得令人难以置信，因为它允许操作符的任意组合，而无需特殊用途的“粘合”代码。
&emsp; 对查询计划的根运算符的get-next调用生成满足查询的元组。为了生成这个元组，根操作符对它的输入操作符执行get-next调用，依此类推到计划树的下面。这通常被称为基于拉取的运算符模型，其中元组基于来自根的初始请求从基本运算符流向根。有关查询处理和迭代器模型的广泛概述，请参见[53]。
&emsp; 由于迭代器模型的一般性质，操作符实现是通用的。 这意味着必须编写操作符代码来处理大量的场景。一个主要示例是处理数据类型:关系运算符在运行时解释元组中的字节，将它们强制转换为适当的数据类型，并根据需要执行运行时错误检查。这避免了为它可能遇到的每个可能的数据类型组合编译运算符的需要。
#### 3.5.2 内存系统中的查询执行   
&emsp; 虽然迭代器模型很好地服务于基于磁盘的系统，但它来自一个I/O是主要开销的时代。当在内存数据库中使用时，迭代器方法带来了一些效率低下的问题。例如，查询为查询管道中生成的每个元组调用get-next调用，这意味着该函数可能被调用数百万次。此外，get-next函数通常是虚拟的，或者通过函数指针调用，这意味着它比普通函数调用更昂贵，并且可能降低现代CPU上的分支预测。此外，由于迭代器模型的泛型性质，它常常导致较差的代码局部性。例如，在处理每个元组时，运算符必须调用另一个函数来解释元组中的字节。在更复杂的情况下，例如，对压缩关系进行表扫描时，运算符必须为记录和解压缩调用每个元组的代码。  
&emsp; 更不用说，这种间接性和较差的代码局部性增加了主内存系统中不必要的开销。正如我们将在第4章中看到的，现代的内存数据库完全避免了迭代器式的这种处理。相反，这些系统积极地将查询和事务编译为机器代码，以避免解释、避免不必要的运行时开销，并有效地利用CPU缓存层次结构。
&emsp; 正如我们将看到的，每个系统在如何编译查询方面略有不同。HIQUE[73]是现代主存数据库时代较早的编译工作之一，它建议使用每个操作符的代码模板将查询编译为C语言。HIQUE通过在运算符执行中内联结果实现来消除迭代器模型。Hekaton使用Microsoft C编译器编译查询和表定义数据。编译表定义允许在编译时知道数据类型，还允许与SQL Server解释查询执行器进行互操作。Hyper通过尽可能长的时间在CPU寄存器中保留记录属性来最大化数据局部性。这是通过编译结束管道断路器的大型管道片段来完成的:物化点需要元组从寄存器溢出。Hyper使用LLVM，其中运算符部分用C++实现，同时生成性能敏感和查询特定的逻辑。
&emsp; 最后，我们注意到查询编译在数据库系统中并不是一个新话题。事实上，最初的System R原型将查询直接编译为机器代码。但是，如[63]中所述，当作为SQL/DS商业化时，System R查询执行器被更改为使用解释器。从那时起，解释查询执行成为基于磁盘的数据库系统中的规范。

## 4.系统  
&emsp; 在本节中，我们将深入研究四个现代内存数据库系统。每个系统的设计和方法都不同，以解决第3章中讨论的问题。我们从4.1节开始，描述了Microsoft SQL Server的主内存OLTP擎Hekaton。然后，我们将在4.2节介绍H-Store及其商业分支VoltDB。第4.3节介绍了TU-Munich的HyPer系统，第4.4节介绍了SAP HANA。最后，我们通过总结第4.5节中其他现代商业和学术主存储系统的设计来总结本章。  
### 4.1 SQL Server Hekaton  
#### 4.1.1引言  
&emsp; Hekaton 是针对集成到 sql server 中的内存驻留数据进行优化的数据库引擎。该系统的官方名称是“In-Memory OLTP”，但通常称为Hekaton。探索和原型设计始于2009年，最初的版本发布在SQL Server 2014中。功能在SQL Server 2016中得到了显着扩展，例如，通过在Hekaton表上添加列存储索引，改进了对实时分析的支持。  
&emsp; 更准确地说, Hekaton 表是由 Hekaton 管理的表, 它完全存储在内存中, 并且可以有多个哈希索引和/或范围索引加上最多一个列存储索引。表是持久的和事务性的，但也支持非持久表。使用T-SQL和基于磁盘的表相同的方式访问它们。查询可以引用Hekaton表和基于磁盘的表，并且事务可以更新两种类型的表。只引用Hekaton表的T-SQL存储过程可以编译为本机机器代码，以进一步提高性能。Hekaton主要针对OLTP应用程序，旨在实现高水平的并发性。数据未分区 - 任何线程都可以访问表中的任何行。该引擎使用无锁存数据结构来避免线程之间的物理干扰，并采用新的乐观多版本并发控制技术来减少事务之间的干扰。数据库可以包含内存表和基于磁盘的表; 只有性能最关键的表需要在内存中。这允许逐步采用新技术，一次一个表和一个存储过程。  
##### 架构原则  
&emsp; Hekaton的设计遵循三个架构原则，旨在实现事务工作负载的低延迟和高吞吐量。
+ 优化内存的数据结构。Hekaton表和索引完全存在于内存中。 行是不可变的 - 每次更新都会创建一个新版本。这使得可以使用数据结构，特别是针对内存优化的索引。例如，对索引中的行的引用可以是直接物理指针。不需要缓冲池，因此完全避免了相关的开销和复杂性。  
+ 非阻塞执行。在多核系统上实现良好的扩展对于高吞吐量至关重要。当系统具有高速更新的共享数据结构（例如锁存器和自旋锁）或高度竞争的资源（例如锁管理器）时，可伸缩性会受到影响。所有Hekaton的内部数据结构，例如内存分配器，索引和事务映射，都完全无锁存（无锁）。系统中任何与性能相关的关键路径上都没有锁存器或自旋锁。Hekaton使用乐观的多版本并发控制，因此没有锁和没有锁表。结果是在一个系统中，线程执行事务而不会停止或等待。  
+ 编译对本机代码的请求。为了最大化运行时性能，仅访问Hekaton表的存储过程可以编译为专用的，高效的机器代码。  

#### 4.1.2数据组织
&emsp; Hekaton表可以有三种类型的索引：使用无锁散列表实现的哈希索引，使用Bw-trees [89]实现的范围索引，以及一个列存储索引。通过索引查找，索引范围扫描，列存储扫描或堆扫描（存储表的存储区的物理扫描）访问行。Hekaton使用多版本化; 更新始终会创建新版本。  
&emsp; 图4.1显示了一个包含六个行版本的简单银行帐户表。暂时忽略数字（100）和红色文本。该表有三个（用户定义的）列：名称，城市和金额。 版本包括标题和许多链接（指针）字段。版本的有效时间由存储在标头的Begin和End字段中的时间戳定义。  
&emsp; 示例表有两个索引，Name上的哈希索引和City上的范围索引。每个索引都需要行中的链接字段。第一个链接字段保留用于Name索引，第二个链接字段用于City索引。为了便于说明，我们假设哈希函数只选择名称的第一个字母。散列到同一存储桶的版本使用第一个链接字段链接在一起。Bw-tree的叶节点存储键和指向记录的指针。如果多个行具有相同的键值，则使用第二个链接字段将重复项链接在一起，并且Bw-tree指向链上的第一行。  
&emsp; 哈希桶J包含四个记录：John的三个版本和Jane的一个版本。Jane的单个版本（Jane，Paris，150）具有从15到无穷大的有效时间，这意味着它是由在时间15提交并且仍然有效的事务创建的。John最古老的版本（John，伦敦，100）在更新时从10时到20时有效。该更新创建了一个新版本（John，London，110）。 我们将在稍后讨论John的最后一个版本（John，伦敦，130）。

&emsp;&emsp;&emsp;&emsp;&emsp;![](/images/Figure4_1.png)

##### 读取  
&emsp; 每个读取操作都指定一个逻辑（as-of）读取时间，并且只有有效时间与读取时间重叠的版本才对读取可见;所有其他版本都被忽略。 行的不同版本具有非重叠有效时间，因此读取时最多只能看到一行的一个版本。例如，查找具有读取时间15的John将触发桶J的扫描，其检查桶中的每个版本但仅返回名称等于John且有效时间10到20的那个。
##### 更新   
&emsp; Bucket L包含两个属于Larry的记录。事务75正在将20美元从拉里的账户转移到约翰的账户。它为Larry（Larry，Rome，150）和John（John，London，130）创建了新版本，并将它们插入到两个索引中。  
&emsp; 注意，事务75已将其事务Id分别存储在新旧版本的Begin和End字段中。字段中的一位表示字段的内容类型。存储在“结束”字段中的事务Id可防止其他事务更新相同版本，并且还可识别正在更新版本的事务。存储在Begin字段中的事务Id通知读者该版本可能尚未提交，并标识创建该版本的事务。  
&emsp; 现在假设事务75提交结束时间戳为100。提交后，事务75返回旧版本和新版本，并将Begin和End字段分别设置为100。最终值在旧版本和新版本下方以红色显示。旧版本（John，London，110）现在的有效时间为20到100，新版本（John，London，130）的有效时间从100到无穷大。Larry的记录以同样的方式更新。  
&emsp; 多版本控制提高了可扩展性，因为读者不再阻止写着。尽管如此，写者仍可能与写者发生冲突。只读事务对更新活动几乎没有影响;他们只是根据需要阅读旧版本的行。多版本化还通过减少行的复制来加速查询处理。由于版本永远不会被修改，因此传递指向它的指针是安全的，而不是制作副本。  
&emsp; 系统必须丢弃不再需要的过时版本以避免填满内存。当某个版本对任何活动事务不再可见时，可以将其丢弃。清除过时的版本，a.k.a。垃圾收集，由所有工作者线程协同处理[38]。  
#### 4.1.3索引  
&emsp; 如前所述，Hekaton引擎中的所有数据结构（包括索引）都是完全无锁存的。Hekaton目前使用静态大小的哈希表，溢出桶实现为latchfree列表。它的范围索引是无锁存的B+-tree（Bw-tree）[89]。Bw-trees中自由锁存的关键是使用映射表将逻辑B +树页面标识符（PID）映射到内存物理页面。映射表是管理 "分页" 树的中心位置。Bw-tree页面之间的所有链接都是PID，而不是物理指针。映射表使Bw-tree节点页面的物理位置能够在每次更新时更改，而不需要将位置更改传播到树的根，因为页间链接是不更改的PID。  
&emsp;&emsp;&emsp;&emsp;&emsp;![](/images/Figure4_2.png)  
&emsp; Bw-tree通过写时复制执行页面更新，而不是通过就地更新（更新现有页面内存）。避免就地更新可以减少CPU缓存失效，这在多插槽机器上尤为重要。减少高速缓存未命中也会增加每个周期执行的指令。增量记录描述了页面P上单个记录的变化（例如，插入，更新，删除）。该增量物理上指向P。更新使用比较和交换（CAS）指令将增量记录的（新）存储器地址安装到映射表中的P的插槽中。如果CAS成功，增量记录的虚拟内存地址将成为页面的新物理“根”地址，从而成功更新页面。增量更新同时实现Bw-tree中的无锁存访问，并通过避免就地更新来保留处理器数据缓存。图4.2（a）描绘了页面P前面的增量更新记录D.虚线表示P的原始地址，而D的实线表示P的新地址。我们偶尔通过创建一个新页面来整合页面，该页面将所有增量更改应用于搜索优化的基页。这减少了内存占用并提高了搜索性能。该页面的合并形式也随CAS一起安装，如图4.2（b）所示，显示页面P及其增量合并为新的“合并页面P”。B+-tree结构修改（页面拆分和删除）也以无锁存方式执行。   
&emsp;&emsp;&emsp;&emsp;&emsp;![](/images/Figure4_3.png)
#### 4.1.4并发控制  
&emsp; Hekaton使用乐观并发控制来提供事务隔离; 没有锁，也没有锁表[77]。悲观并发控制通过锁定来防止冲突。乐观并发控制不会尝试防止冲突，而是通过在提交之前验证更新事务的读取来检测何时发生冲突。如果验证失败，则事务中止。  
&emsp; 事务可以处于以下四种状态之一：活动，准备，提交或中止。图4.3显示了状态之间可能的转换。 事务在其生命周期中经历三个不同的阶段。
1. 事务被创建; 它获取一个开始时间戳并将其状态设置为“活动”。  
2. 正常处理阶段。该事务在此阶段执行所有正常处理。事务永远不会在此阶段阻塞。对于更新操作，事务将其事务ID复制到新版本的Begin字段以及旧版本或已删除版本的结束字段中。如果中止，则将其状态更改为“已中止”并直接跳至步骤4。当事务完成其正常处理并请求提交时，它将获取结束时间戳并切换到“准备”状态。
3. 准备阶段。在此阶段，事务执行验证以确定它是否可以提交或被强制中止。如果必须中止，则将其状态切换为Aborted并继续下一阶段。如果它已准备好提交，它会将所有新版本和有关已删除版本的信息写入重做日志记录，并等待日志记录到达稳定存储。然后事务将其状态切换为Committed。  
4. 后处理阶段。如果事务已提交，则继续从新版本的Begin字段和旧版本或已删除版本的End字段替换其事务ID及其结束时间戳。如果事务已中止，则会将其所有新版本标记为垃圾。   
5. 事务现已终止。当事务的旧版本不再对任何活动事务可见时，他们分配给垃圾收集器，垃圾收集器负责物理删除它们。  

&emsp; 时间戳来自全局的，单调递增的计数器。事务通过原子读取和递增计数器获得唯一的结束时间戳。  
&emsp; 准备阶段的验证程度取决于事务的隔离级别。无论隔离级别如何，只读事务和在快照隔离下运行的更新事务都不需要验证。 当事务尝试更新版本并导致事务回滚时，会立即检测到写-写冲突。  
&emsp; 在可重复读取或可序列化隔离下运行的事务在提交之前需要验证。在验证期间，事务T检查以下两个属性是否成立。
+ 读稳定性。如果T在处理过程中读取了某个版本V1，我们必须确保V1在事务结束时仍然是T可见的版本。这是通过验证在T提交之前V1尚未更新来实现的。任何更新都会修改V1的结束时间戳，因此所需要的只是检查V1的结束时间戳。为了实现这一点，T保留了指向它已读取的每个版本的指针。   
+ 避免幻影。对于可序列化的事务，我们还必须确保事务的扫描不会返回其他版本。这是通过重新扫描以在提交之前检查新版本来实现的。为了实现这一点，可序列化事务保留其所有索引扫描的跟踪并保留足够的信息以便能够重复每次扫描.

#### 4.1.5查询处理
&emsp; 使用普通T-SQL通过本机编译的存储过程或通过查询互操作来访问内存表。为了获得最大速度, 仅访问内存中表的存储过程将被编译为自定义的高效机器代码。首先将T-SQL过程转换为C代码，该代码由Microsoft C编译器生成，然后生成DLL，然后将其加载到SQL Server进程中。生成的代码完全包含执行请求所需的内容，仅此而已。尽可能多的决策是在编译时进行的，以减少运行时开销。例如，所有数据类型在编译时都是已知的，允许生成有效的代码。  
&emsp; 经典的SQL Server引擎可以通过为此目的构建的特殊运算符访问或更新内存表。例如，存在索引扫描运算符，用于对Hekaton表上的索引执行查找或范围扫描。调用者指定搜索键或键范围，并且运算符输出具有经典引擎使用的内部行格式的请求列的限定行。这种互操作能力至关重要;例如，它用于临时查询，用于组合来自基于磁盘的表和内存表中的数据的查询，以及需要在本机编译的存储过程中不可用的功能的查询。  
#### 4.1.6持久性和恢复  
&emsp; 通过将日志和检查点移到外部存储来确保事务持久性。仅记录用户数据，而不记录索引。在恢复期间，Hekaton表及其索引完全从最新的检查点和日志尾部重建。  
&emsp; 成功通过验证的事务已准备好提交。此时，它会将日志创建的所有新版本以及已删除的所有版本的键写入日志。这是在一次写入中完成的（非常大的事务除外），如果写入成功，则事务不可撤销提交。不会记录中止的事务, 因此中止事务的成本很低。
##### 检查点  
&emsp; 为了缩短恢复时间，Hekaton还实现了检查点。检查点方案旨在满足三个关键要求。

+ 连续检查点。与检查点相关的I/O应根据需要以增量方式和持续方式进行, 以避免突然出现对事务性工作负载产生负面影响的I/O峰值。   
+ 顺序I/O。对于大多数操作，检查点应该依赖于顺序I/O而不是随机I/O.即使在SSD设备上，随机I / O也比顺序慢，并且可能导致更多的CPU开销。
+ 并行恢复。在恢复期间将数据加载到内存中应高度并行化，以充分利用可用的I / O带宽并最大限度地缩短恢复时间。

&emsp; 检查点数据存储在两种类型的检查点文件中：数据文件和增量文件。一个完整的检查点由多对数据和增量文件组成。数据文件包含在特定时间戳范围内创建的所有新版本。数据文件仅在打开时附加，一旦严格只读即可关闭。在恢复时, 数据文件中的版本将重新加载到内存中并重新编制索引, 但须通过增量文件进行过滤。  
&emsp; 增量文件存储有关随后删除其关联数据文件中包含的版本的信息。增量文件也是仅附加的。在恢复时，增量文件用作过滤器，以避免将已删除的版本重新加载到内存中。将一个增量文件与每个数据文件配对的选择意味着恢复的最小工作单元是数据/增量文件对，从而导致高度可并行化的恢复过程。  
&emsp; 完整的检查点与事务日志的尾部相结合，可以恢复Hekaton表。检查点具有时间戳，该时间戳指示检查点时间戳之前的所有事务的影响都包括在检查点中，因此无需从事务日志中恢复。  
&emsp; 检查点任务获取前一个检查点未覆盖的事务日志的一部分，并将日志内容转换为一个或多个数据文件并更新为增量文件。新版本将附加到最新数据文件或新数据文件中，已删除版本的ID将附加到与存储原始插入版本的数据文件对应的增量文件中。  
&emsp; 检查点中涉及的文件集随每个检查点而增长，但数据文件的活动内容会随着越来越多的版本在其增量文件中被标记为已删除而降级。由于崩溃恢复会读取检查点中所有数据和增量文件的内容，因此恢复性能会随着每个数据文件的实用程序的下降而降低。当相邻的数据文件的活动内容 (数据文件中未删除版本的百分比) 低于阈值时, 暂时合并相邻的数据文件时可避免此问题。合并两个数据文件DF1和DF2导致新的数据文件DF3覆盖DF1和DF2的组合范围。合并期间将删除所有已删除的版本，因此新的增量文件在合并后立即为空。

#### 4.1.7 性能和进一步说明
##### 性能问题
&emsp; 与传统的基于磁盘的数据库系统相比，内存数据库可提供更高的吞吐量和更低的延迟.如果性能改进足够大, 这将产生真正的影响。下面的示例说明了一种这种情况。  
&emsp; EdgeNet为供应商，零售商和搜索引擎提供优化的产品数据。为了在繁重的工作负载下优化性能，应用程序将从缓存中返回产品信息，而不是查询数据库。因为该公司从多个来源收到大量数据，如果不与读取进程和锁定事务冲突, 则无法直接写入生产数据库。相反，他们将文件接收到临时环境中，在加载到数据库之前对数据进行转换和结构化。可能需要一天时间来准备和加载信息，以便Edgenet寻找替代方案。他们想要一种在线交易处理（OLTP）解决方案，为客户提供实时的信息访问。  
&emsp; 通过切换到Hekaton，EdgeNet可以允许读取和写入活动在同一数据库上并发运行，从而实现连续的数据接收。他们不再需要等待一天甚至一小时来筹划和准备数据。此外，它们还可以消除前端缓存层和用于数据加载的暂存区域。总之，从切换到内存数据库获得的实质性性能够实现更好的客户服务和简化的系统配置。   
##### 进一步说明
&emsp; 参考文献[38]提供了最全面的Hekaton设计概述，包括数据存储和索引，并发控制，查询处理和编译，以及日志记录，检查点和恢复。[89]中提供了用于范围索引的Bw-tree的详细描述。[77] 中详细介绍了并发控制算法。[47] 中还介绍了编译过程。

### 4.2 H-Store与VoltDB
#### 4.2.1 引言
H-Store(商业版叫VoltDB)是一个旨在有效执行现代OLTP工作负载环境下的数据库管理系统。除了使用内存作为数据库的主要的存储介质之外，它的体系结构基于以下四个设计原则：
- **分区和串行执行**：除了允许事务的并发执行外，H-Store还会对数据进行分区并在每个分区上串行的执行事务。[[81](),[90](),[135](),[157]()]。这意味着它不需要使用重量级并发控制方案来管理细粒度锁。当一个事务执行时，它从不需要暂停，因为它所需要的所有数据都已经在内存中，并且它不会等待获取另一个事务持有的锁。
- **存储过程执行**：应用程序不是在运行时发送SQL命令，而是在H-Store中注册一组基于SQL的进程，并且只通过这些进程调用事务。将所有事务逻辑封装在存储过程中可以防止应用程序在事务中间停顿，并且还可以避免运行时执行查询计划的开销。虽然这种模式要求事先知道所有事务，但这种假设对于OLTP应用程序[[142](),[96]()]是合理的。
- **分布式部署**：为了支持大于单个节点上可用内存量的数据库，H-Store将跨无共享[[142]()]计算节点将数据库拆分为称为分区[[36](),[26]()]的不相交段。分区可以跨多个节点复制，着为事务处理系统提供了高可用性和容错性。
- **精简日志**：为了避免恢复机制的需要非常大的开销[[102]()]，H-Store使用了一个轻量级的逻辑日志记录方案，只需要记录运行的事务，而不需要记录事务对数据库所做的内存上的数据更改。

![figure-4-4](./images/figure4-4.PNG)


多年来，H-Store系统已经有了多个版本。最初的概念版是2007年在麻省理工学院开发的单节点引擎，它只能执行TPC-C基准测试的简化版本[[142]()]。H-Store的全功能通用版本由Brown，MIT，Yale和Vertica Systems于2008年开发。2009年，这个版本的H-Store建立了一个商业化的分支VoltDB。2010年，在VoltDB的一些更新也被同步的H-Store的源代码中，但是运行时事务管理子系统和协调子系统的部分被重写。 从那时起，H-Store在学术界上继续作为研究试验的平台，而VoltDB继续开发对实际部署具有重要意义的功能。

H-Store的架构分为两部分:(1) 前端事务管理组件和(2) 后端查询执行程序和存储管理器。 前端由用于与应用程序的客户机、事务协调器和存储过程通信的所有网络库组成。系统的这一部分是用Java编写的。每个分区的后端c++执行引擎包含存储管理器、索引和查询计划执行器。

#### 4.2.2 数据组织
图4.4中的图表显示了H-Store中表和索引的存储布局。在一个节点上的所有执行进程都在相同的地址空间中运行，但是它们的底层分区不共享任何数据结构。每个分区为数据库表维护单独的索引，这些表只包含与该特定分区关联的元组的条目。这意味着执行引擎无法访问存储在同一节点的另一个分区中的数据。表的内存存储区被拆分为固定大小的块和可变长度的块池。固定大小的块池是表元组的主存储空间。所有元组的大小都是固定的，以确保它们字节对齐。表中大于8字节的任何字段都单独存储在可变长度的内存块中。该块的8字节内存位置存储在该字段在元组中的位置[[135]()]。小于8字节的所有其他字段都是在线存储的。每个元组前面都有一个1字节的字段，其中包含有关当前事务是否修改或删除过元组的数据。此字段用于H-Store的快照机制（第4.2.6节）。

DBMS将块ID号的查找表维护到相应的内存位置。这个查找表允许执行引擎使用表存储区域中的4字节偏移量而不是8字节指针引用单个元组。也就是说，从4字节的偏移量开始，存储层可以用元组计算块的ID以及该块中元组的位置。

DBMS将未排序的表元组存储在存储块中。对于每个表，DBMS维护未占用（即空闲）元组的4字节偏移量列表。当事务删除元组时，删除的元组的偏移量将添加到此池中。当事务向表中插入元组时，DBMS首先检查表的池，以查看是否有可用的元组。如果池是空的，那么DBMS将分配一个新的固定大小块来存储要插入的元组。此插入操作不需要的附加元组偏移量因为插入操作作用在表的可用元组池中。如果从表中删除了大量元组，则H-store不会压缩块。

在H-Store上部署新应用程序之前，管理员必须向DBMS的项目编译器提供（1）数据库的模式，（2）应用程序的存储过程，以及（3）数据库的设计规范。编译器将生成一个目录，其中包含应用程序数据库中组件的元数据（如表、索引、约束）以及每个应用程序存储过程的已编译查询计划。

##### 数据库设计
应用程序的数据库设计规范定义了数据库的物理配置，例如是将一个特定的表划分为多个分区，还是在每个节点上复制它。这将确定每个事务在运行时访问的分区。设计模式确定H-Store是将来自应用程序的事务请求作为一个快速的单分区事务执行，还是作为一个缓慢的分布式事务执行。也就是说，如果表在节点之间划分，事务基础分区拥有事务所需的所有数据，那么它就是单分区的。确定任意应用程序的最佳配置并非易事，特别是对于具有许多依赖项的复杂企业应用程序。有大量关于最小化分布式事务和倾斜量的自动数据库设计的研究。[33,120]
- **表分区**：一个表可以水平地分为多个不相交的片段，它们的边界基于表的一个(或多个)列的值（例如分区属性）[160]。DBMS使用范围分区或散列分区，根据这些属性的值将每个元组分配到特定的片段。来自多个表的相关片段被组合到一个分区中[51,118]。OLTP应用程序中的大多数表将以这种方式分区。在图4.5(a)的示例数据库中，CUSTOMER表中的每个记录都有一个或多个ORDERS记录。因此，如果两个表都在根据WAREHOUSEid分区（例如CUSTOMER.W_ID 和ORDERS.W_ID），然后，无论数据库的状态如何，只访问单个仓库中的数据的所有事务都将作为单分区执行。
- 
![figure4-5](./images/figure4-5.PNG)

- **表复制**: 除了把表分成多个分区之外，数据库还可以跨分区复制表。表复制对于只读或读(主要是与其他表一起访问但不共享外键祖先的表)非常有用。这与为了持久性和可用性复制整个分区是不同的。例如，图4.5(b)中的只读项表与CUSTOMER表没有外键关系。通过复制这个表，事务不需要从远程分区检索数据来访问它。但是修改复制表的任何事务都必须作为分布式事务执行，该分布式事务锁定集群中的所有分区，因为这些更改必须广播到集群中的每个分区。除了避免额外的分布式事务之外，还必须考虑在每个分区复制表所需的空间。

#### 4.2.2 索引
对于惟一索引和非惟一索引，H-Store支持哈希表和b树数据结构。索引中的条目值是元组的偏移量。由于分区内的执行是单线程的，因此索引实现不需要是线程安全的。这将导致更简单的索引实现和更短的代码路径。

![figure4-6](./images/figure4-6.PNG)

##### 二级索引的复制
当查询使用不属于该表分区属性的列访问表时，它将广播到所有分区。这是因为DBMS不知道哪个分区具有查询所需的元组。然而，在某些情况下，如果数据库包含跨所有分区复制的表列子集的辅助索引，则这些查询可能成为单分区查询。考虑图4.6所示数据库的一个事务，该事务执行一个查询，使用CUSTOMER的last name检索CUSTERM的id。如果每个分区包含一个具有id和last name列的辅助索引，那么DBMS可以自动重写存储过程查询计划，以利用这个数据结构，从而使更多的事务单分区。只有在这些索引中选择的列不经常更新的情况下，这种技术才能提高性能。

#### 4.2.4 并发控制
与在分区[17]上同时执行多个事务的重量级并发控制方案不同，H-Store在每个分区上一次执行一个事务。也就是说，当事务在H-Store中执行时，它对所需分区上的数据和索引具有独占访问权。事务不会因为等待获取另一个事务持有的锁而停止，因为在它的基本分区或远程分区上不会同时运行任何其他事务。

H-Store基于时间戳调度事务[17]。当事务请求到达节点时，协调器根据其到达时间戳为请求分配唯一标识符id。这个id是一个组合键,由三部分组成，其一为当前节点的墙上时钟时间(以毫秒为单位),其二为次的墙上时钟时间已经到达的事务数量的计数器(如果多个事务在同一时间进入系统),和事务的基础分区id [149]。

每个分区都由一个由其协调器管理的锁保护，该锁根据事务ID的顺序一次授予事务[5、17、32、157]。如果（1）事务的最低ID不大于授予该锁的最后一个事务的最小ID，并且（2）自事务首次进入系统以来已至少5 ms，则事务获取分区S锁[142]。这个等待时间确保了通过网络向远程分区发送锁获取消息的分布式事务不会受到限制。我们假设使用标准的时钟偏移算法来保持每个节点上不同的CPU时钟同步。

以这种方式在每个分区序列化事务对于OLTP工作负载有几个好处。在这些应用程序中，大多数事务一次只访问数据库中的单个实体（例如，在单个客户上操作的事务）。这意味着，如果数据库的分区方式使大多数事务只访问一个分区，那么DBMS的执行速度比传统的DBMS快得多。Smallbase是这种方法的早期支持者[65]，最近的例子包括k[157]和granola[32]。然而，这种方法的缺点是，它意味着需要在两个或多个分区上访问数据的事务明显较慢。如果一个事务试图访问它不持有锁的分区上的数据，那么DBMS会中止该事务（释放它持有的所有锁），恢复任何所作的更改，然后在事务重新获取它需要的所有锁后重新启动它。采用这种方法可以消除分布式死锁检测的需要，从而为OLTP应用程序中的短期事务提供更好的吞吐量[60]。

协调器在包含事务将要访问的分区的所有节点处对请求进行排队。当事务获取分区的锁时，协调器准备一条确认消息，以将其发送回事务的基本分区。一旦事务从节点的分区获取了它需要的所有锁，协调器就会发送这个确认。一旦一个事务接收到它所需要的分区的所有锁确认，它的基本分区的协调器就会安排事务立即在它的基本分区的执行引擎上运行[18，24]。

#### 4.5.4 查询处理
H-store中的每个分区都由单线程的执行引擎管理，该引擎对该分区上的数据具有独占访问权。执行引擎由两部分组成，一部分是用Java编写的，另一部分是用C++编写的。在Java级组件中，执行引擎的线程在队列中阻塞，等待代表事务执行工作的消息。此工作可以指示引擎调用过程的控制代码以启动新事务，或者代表在另一个分区上运行的事务执行查询计划片段。注意，对于后者，H-Store的事务协调框架确保不允许事务在执行引擎上排队查询请求，除非事务持有该引擎分区的锁。

在执行引擎的c++ 库中，H-Store存储数据库并处理查询。Java层使用Java Native Interface (JNI)框架调用c++ 库中的方法，并将事务调用的查询计划标识符传递给它。此库不知道集群中的其他分区或节点;它只对所提供的输入进行操作。

![figure4-7](./images/figure4-7.PNG)

##### 存储过程
每个存储过程由唯一的名称标识，并由调用预定义参数化SQL命令的用户编写的Java控制代码(例如应用程序逻辑)组成。应用程序通过向DBMS发送一个包含过程名称和集群输入参数的请求来启动事务。这些存储过程的输入参数可以是标量值或数组基元值。

如图4.7中的示例所示，存储过程具有包含该过程的应用程序逻辑的run方法。在H-Store中没有显式的事务开始或提交命令。当其基本分区的执行引擎调用此方法时，事务开始，然后在此方法返回时完成(通过返回或中止命令)。当此控制代码执行时，它在运行时发出查询调用请求，方法是将目标查询句柄和该调用的输入参数传递给H-Store运行时的API（例如queueSQL）。这些输入参数的值将被替换为查询的参数占位符（在图4.7中的SQL语句中表示为？）。DBMS将每次调用进行排队，然后立即返回到控制代码。即使使用相同的输入参数，对同一查询的多个调用也被视为独立的。将所有需要的调用请求添加到当前批处理后，控制代码指示DBMS分派批处理以进行执行操作（例如executeBatch）。此时，控制代码被阻塞，直到DBMS完成当前批处理中的所有查询或由于错误而中止事务为止（例如如果其中一个查询违反完整性约束）。此命令返回上次执行的批处理中每个查询调用的输出结果的有序列表。

H-Store包括内置于DBMS中的特殊系统存储过程。这些过程允许用户执行系统中的管理功能，例如将数据批量加载到表中、修改配置参数和关闭集群。

虽然H-Store中的存储过程包含任意用户编写的Java代码，但我们要求它们的所有操作和副作用都是确定性的。 也就是说，必须编写每个存储过程，使得如果DBMS使用相同的输入参数并且以相同的顺序（相对于其他事务）再次执行事务，则该事务完成之后的数据库的状态将是相同的。 这意味着不允许过程的控制代码在再次执行时产生不同结果的操作。 此要求是H-Store恢复所必需的（第4.2.6节）

存储过程控制代码中禁止的非确定性操作的类型包括（1）使用过程内的RPC库与外部系统通信，（2）从节点的系统时钟中检索当前时间，或（3）使用随机数发生器。作为这些问题的一个例子，考虑一个与外部第三方欺诈检测系统联系的程序，以确定在交易过程中财务转移是否是欺诈性的。然后，事务将根据此系统的响应选择是提交还是中止。在H-Store事务中执行此操作的一个问题是，如果通过在不同节点（即，复制的部署）上运行的相同事务的不同调用多次向其发送相同的请求，则该服务可以报告误报。另外，当DBMS重放事务（即，崩溃恢复）时，该服务可能在以后不可用。在任何一种情况下，数据库都将不一致。因此，为了使该应用程序在H-Store中可靠地工作，开发者将需要在该过程之外进行移动欺诈检测操作。

##### 存储过程的路由

除了分区或复制表之外，数据库设计还可以确保将每个事务请求路由到具有所需数据的分区（即其基本分区）[126]。 H-Store使用在运行时设计中定义的过程路由属性将新的事务请求重定向到将要执行它的节点[112]。每个过程的最佳路由属性使DBMS能够识别哪个节点具有每个事务所需的最多（如果不是全部）数据，因为这允许它们可能以减少的并发控制执行[120]。图4.8显示了如何根据与CUSTOMER表的分区属性对应的输入参数的值来路由事务。如果事务在一个节点上执行但其所需的数据在其他地方执行，那么它必须以完全并发控制执行。对于许多应用程序来说这很困难，因为它需要使用基于工作负载的近似或静态代码分析将过程输入参数映射到其查询输入参数。

#### 4.2.6 持久化与恢复
由于H-Store是内存数据库，因此必须确保所有事务的修改都是持久的，并且在节点崩溃时可以恢复。 然而，关键是提供这种保证而不会明显的阻碍系统的性能优势。 由于H-Store设计为在商用硬件上运行，我们不能假设DBMS将使用专用组件（例如，备用电池的内存）进行部署[49]。

鉴于此，H-Store使用轻量级逻辑日志记录方案，与现有的面向磁盘的系统相比，其开销更少[102]。 它还将采用数据库的定期检查点来减少崩溃后系统的恢复时间。 图4.9描述了该过程的概述。 我们现在讨论这两种机制。 然后，我们将在4.2.6节讨论H-Store如何从这些日志和检查点恢复数据库的状态。

![figure4-8](./images/figure4-8.PNG)

##### 命令日志
如3.4节所述，日志有物理日志和逻辑日志等日志形式。 H-Store使用逻辑日志的变体，称为命令日志，其中DBMS仅将事务调用请求记录到日志[96]。 每个日志记录包含存储过程的名称和从应用程序发送的输入参数以及事务的标识。 因为一个日志记录代表整个事务调用，所以命令日志记录不支持事务保存检查点[103]。 这不是一个重要的限制，因为OLTP事务是短暂的。

H-Store使用单独的线程保存日志记录;执行引擎永远不会被日志记录操作阻止。 DBMS在事务执行之后将结果返回给应用程序之前写入条目。这与预写日志[59]不同，它的DBMS在执行之前会记录事务请求。这有两个好处。第一个是DBMS不需要为中止的事务写出条目，因为系统确保在执行下一个事务之前首先回滚中止事务所做的所有更改。因此，中止事务对数据库状态的影响与从未执行过时相同。其次，由于H-Store可以由于内部控制机制而重新启动事务，因此可以为事务分配多个事务标识符。例如，当事务尝试访问它没有锁定的分区时，DBMS将重新启动该事务，回滚它所做的任何更改，为其分配新的事务标识符，然后在它需要的分区重新提交锁定获取请求。如果DBMS在执行之前记录了事务的命令，则每次重新启动事务时都需要写入一个新条目，该条目将前一个条目标记为无效。

DBMS将命令日志条目组合在一起以进行多个事务，并将它们批量写入以分摊写入磁盘的成本[36,61,157]。 在刷新日志记录之前，应用程序不会看到事务所做的修改。同样，在将事务之前执行的所有事务写入命令日志之前，无法将事务释放到应用程序。

##### 快照
当DBMS执行事务并将其命令写入日志时，DBMS还会创建数据库表[90,142]的非阻塞快照。 每个分区的快照都写入其主机节点的本地磁盘。 当系统在崩溃后需要恢复数据库时，它会在最后创建的检查点中加载，然后只重放在此检查点之后出现在命令日志中的事务[96]。 这大大减少了恢复数据库所需的时间。 H-Store的快照仅包含表中的元组，而不包含索引。

可以将DBMS配置为使用系统存储过程定期或手动获取检查点。 系统还维护一个快照的目录，该目录可由应用程序通过另一个系统过程检索。

当H-Store启动新检查点时，DBMS中的一个节点被选为下一个检查点的协调节点。该节点可以随机选择（如果它是由DBMS发起的预定检查点），也可以是具有事务基本分区的节点（如果它是通过系统过程启动的）。此节点上的DBMS向集群中的每个分区发送特殊事务请求，以指示它们开始检查点进程。此请求锁定所有分区，以确保每个节点从事务一致的数据库状态开始编写检查点[122]。该系统过程使所有分区的每个执行引擎切换到特殊的“写时复制”模式。未来事务所做的任何更改都不会覆盖当前检查点启动时存在的元组，并且在检查点启动后插入的任何新元组都不会包含在快照数据中。一旦所有分区都发回确认，DBMS就会提交特定的事务，并且每个分区都会在单独的线程中开始将快照写入磁盘。然后，执行引擎返回处理事务，而快照处理在后台进行。

分区上的执行引擎将数据库快照完成到磁盘所花费的时间取决于数据库的大小和存储设备的写入速度。 在分区引擎完成快照写入后，它会禁用“写时复制”模式并将通知消息发送回协调节点。 一旦协调节点收到来自每个分区的通知，它就会向每个分区发送最终完成消息，指示它们清理瞬态数据结构并将快照标记为完成。
##### 故障恢复
从命令日志和快照还原数据库的过程很简单[96]。 首先，当H-Store节点启动时，节点上的每个执行引擎都会读取其分区上最后一个快照的内容。 对于快照中的每个元组，DBMS必须确定应该存储该元组的分区，因为它可能与在该快照中读取的分区不同。 如果管理员在系统脱机时更改群集中的分区数，则可能发生这种情况。 当引擎加载到每一行时，它还将在其分区上重建表索引。

一旦快照从磁盘上的文件加载到内存中，DBMS将重放命令日志以恢复数据库，以恢复它在崩溃之前所处的状态。 一个单独的线程向后扫描日志以查找与发起刚刚加载的快照的事务相对应的记录。然后在那个点上从此点向前扫描日志并重新提交每个条目作为新的事务请求。 事务协调器确保这些事务按照它们到达系统的确切顺序执行; 这与允许重新排序和重新执行事务的正常执行策略不同。

即使重放期间的分区数与运行时的分区数不同，也可确保此恢复过程之后的数据库状态正确。 这是因为（1）事务以串行顺序记录和重放，因此重新执行的顺序与初始执行完全相同，并且（2）重放从不包含任何未提交数据的事务一致性快照开始，因此在恢复时不需要回滚[59,122,96]。

### 4.3Hyper  
#### 4.3.1 引言 
&emsp; 内存数据库系统HyPer是在慕尼黑技术大学开发的，用于在相同的数据库状态下提供混合OLTP和OLAP工作负载，并提供完整的ACID保证。这允许对最新事务数据库状态进行OLAP数据探索；从而实现经常引用的“实时分析”。传统上，两个工作负载OLTP和OLAP在两个专用系统中分开：事务数据库有规范化的框架并且数据仓库用于以分析为主的查询处理。由于数据刷新之前的时间延迟，必要的ETL过程会引起数据仓库的复杂性和陈旧性问题。  
&emsp; 更强大的数据库服务器的出现，即使是具有许多核心和几TB内存的容量的商用服务器，也最终为在一个系统实例中整合这两个异构工作负载铺平了道路。  
#### 4.3.2数据组织
##### 通过快照隔离OLTP事务和OLAP查询  
&emsp; 为了在同一数据库状态下同时容纳OLTP和OLAP工作负载，有必要将这两个任务有效地隔离开来。一种可能的更新暂存方法是它在包含较旧的，主要是不可变数据对象的主数据库系统和包含最近插入和更新的数据对象的增量存储之间分隔数据。增量存储会定期合并到内存中。更新暂存方法的缺点是为了将增量与主数据库系统合并而产生额外的重组工作, 这导致我们设计了一种替代体系结构, 在这种体系结构中, 事务数据库完全保持在一致的状态。  
&emsp;&emsp;&emsp;&emsp;&emsp;![](/images/Figure4_10.png)    
&emsp; HyPer利用操作系统功能为新的重复进程创建虚拟内存快照。例如，在Unix中，这是通过fork（）系统调用创建OLTP进程的子进程来完成的。为了保证事务一致性，fork（）应该只在两个（串行）事务之间执行，而不是在一个事务的中间执行。通过使用撤消日志将操作一致性快照（在事务中间创建）转换为事务一致性快照，可以放宽此约束。   
&emsp; fork子进程获得父进程地址空间的精确副本，如图4.10左侧所示的覆盖页面框架面板所示。fork（）操作创建的虚拟内存快照将用于执行OLAP查询 - 如图4.10右侧所示。  
&emsp; 快照保持精确处于fork（）发生时存在的状态。 幸运的是，最先进的操作系统不会立即物理复制内存段。相反，他们采用了一种懒惰的写时复制/更新策略 - 如图4.10右侧所示。最初，父进程（OLTP）和子进程（OLAP）通过将虚拟地址（例如，对象a）转换到相同的物理主存储器位置来共享相同的物理内存段。内存段的共享在图形中通过虚线框突出显示。虚线框表示尚未（尚未）复制的虚拟内存页面。只有当对象 (如数据项 a) 被更新时, os 和硬件支持的复制更新机制才会启动所驻留的虚拟内存页的复制。此后，存在由OLTP进程可访问的新状态，其表示执行事务，并且旧状态表示为a，可由OLAP查询会话访问。与图所示不同的是, 附加页面实际上是为启动页面更改的OLTP过程创建的, OLAP快照引用旧页面-如果创建了多个此类快照, 此详细信息对于估计空间消耗非常重要。此快照机制通过操作系统与内存管理单元（MMU）的组合，将OLTP事务处理与OLAP查询评估完全分开。
##### 混合存储结构  
&emsp; 如第3章所述，HyPer避免了特定于数据库的缓冲区管理和页面结构。数据驻留在虚拟存储器内的简单的内存优化数据结构中。因此，HyPer以“全速”利用OS / CPU实现的地址转换，而无需任何额外的间接手段。即使虚拟内存可以（显着）超过物理内存，我们也会将数据库限制为物理内存的大小，以避免操作系统控制的虚拟内存页面交换。  
&emsp; 对于在虚拟内存中组织关系, 在设计空间的边界上有两个众所周知的极端:在行存储方法中，关系被维护为整个记录的数组，并且在列存储方法中，关系被垂直划分为属性值的向量。HyPer可以配置为作为列或行存储 - 但表格布局也可以根据访问模式进行调整。在混合存储格式中，可以将频繁访问的那些属性聚类以构成非冗余垂直分段的单个向量。  
&emsp; 假设经常发生以下查询：  
&emsp; select oDate, sum(oPrice)  
&emsp; from Orders   
&emsp; where oDate >= 20130101  
&emsp; group by oDate  
&emsp; 然后将两个属性oDate和oPrice聚类到同一个向量中将是有益的。下面我们展示了这样的存储布局（为简单起见，该示例使用C ++ / STL进行矢量集合，尽管HyPer使用了自己的数据结构）：  
///订单  
struct Order {&emsp;unsigned id;   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; unsigned customer;   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; unsigned oDate;  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; double oPrice;   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; };  
struct OrderDatePrice {&emsp;unsigned oDate;   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;double oPrice;  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;   
///混合格式的所有订单  
struct Orders {
vector<unsigned> data_id;  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;vector<unsigned> data_customer;  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;vector<unsigned> data_product;  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;vector<OrderDatePrice> data_oDate_oPrice;  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;void insert(Order&& order);  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;};   
&emsp; 查询的代码生成.然后可以将上面显示的示例查询转换为以下C ++代码，该代码依赖于对一个集群向量data_oDate_oPrice的扫描：   
unordered_map<unsigned, double> revenueByDate(Orders& orders)  
{  
&emsp;&emsp;&emsp;&emsp;unordered_map<unsigned, double> groupBy;  
&emsp;&emsp;&emsp;&emsp;for (OrderDatePrice date_price : orders.data_oDate_oPrice) {  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;if (date_price.oDate >= 20130101) {  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;groupBy[date_price.oDate] += date_price.oPrice;  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;}  
&emsp;&emsp;&emsp;&emsp;}  
return groupBy;  
}  
&emsp; 该程序片段简化了HyPer查询引擎的许多方面;然而，它旨在证明翻译声明性SQL查询确实可以产生与手写代码一样快（或由于多核并行化甚至比手写代码快得多）的可执行代码。  
&emsp; 实际的HyPer引擎在某些主要方面偏离了这个简单的代码生成：   
1. HyPer采用成熟的高级查询优化器来优化连接顺序，取消嵌套子查询，谓词下推等。  
2. HyPer将用HyPerScript编写的SQL查询和事务程序编译为LLVM汇编程序代码。最重要的优点是消除了需要几秒钟的C ++编译器，从而阻碍了交互式查询处理。  
3. HyPer自动并行化查询执行，以充分利用多核服务器，如今，它拥有数十个或数百个核心。
4. HyPer采用复杂的数据结构和算法，这些数据结构和算法特别具有高速缓存意识，并且可以为线程级并行化带来低同步开销。  
&emsp;&emsp;&emsp;&emsp;![](/images/Figure4_11.png)  

&emsp; 动态存储分配。HyPer最初假设关系仍然很小并且仅分配固定（小）向量 - 以避免动态分配存储的簿记开销。但是，一旦关系增长超过某个阈值，动态增长的分区向量将通过直接映射表DM进行寻址，如图4.11所示，用于HyPer。一个水平片段的所有数据向量连续存储。如果使用纯列式格式，则连接所有属性向量。在我们的示例中，我们选择了一个聚类数据向量，该向量恰好存储在片段的末尾。每次片段溢出时，都会分配一个新片段，使（可能的）行的总数加倍。因此，图中的前两个片段有两行空间，下一行为四行，然后为八行，等等。访问特定位置的属性，比如检索行007的Product的属性值分两步完成：  
1. 通过直接映射表DM查找片段的起始地址。DM中的地址是通过相当有效的转换找到的, 方法是计算64位行号的前导零, 并从DM中的位置数中减去该地址。在示例中, 这将生成直接映射表中的标记位置。  
2. 检索到相应片段的起始地址（示例中的第三个片段）后，地址计算产生属性值的标记位置。根据片段中的行数和属性的宽度, 通过预先计算实例化表优化此地址计算。 
&emsp; 通过直接映射表的间接引起的开销很小，因为它足够小以适应L1高速缓存，并且所有地址转换都由预实现支持。   
#### 4.3.3索引  
&emsp; HyPer使用新的内存索引，该索引依赖于搜索关键字的基数分段。因此，大多数可以使用无分支和无比较的代码来导入从根到叶的基数树索引。树的每个级别都维护搜索关键字的特定基数片段 - 在我们的例子中是一个字节。在HyPer之前，基数树（通常称为尝试）由于在每个节点中保持最大扇出度而遭受差的存储利用率。与平衡搜索树相比，像AVL-或红/黑树尝试具有“漂亮”属性，其高度不依赖于索引对象的数量。相反，搜索键的长度决定了基数树的高度。  
&emsp;HyPer基数树设计，称为自适应基数树ART [85]，根据节点的实际扇出使用自适应节点大小，以保证良好的空间利用率。因此, 一个节点开始时的空间很小, 为四向扇出, 然后增长到16路扇出的大小。如果插入了更多的搜索键，它会增长到48路扇出，并最终变为扇出256。  
&emsp; 如上所述，ART采用四种自适应大小的节点类型：
+ 节点4:此节点类型最多存储四个已排序的搜索键, 最多可以输入四个风扇。  
+ 节点16：此处，最多16个搜索键按排序顺序存储。  
+ 节点48：在此节点类型中，使用256个元素的数组，每个搜索键有一个条目。这些条目构成指向要找到“真实”子指针的48个位置之一的短（即一个字节）指针。
+ 节点256：这里使用256元素数组，其中条目包含指向子节点的指针。

&emsp; 图4.12提供了一个自适应节点类型的示例，它显示了一个从根到叶子的ART树的样本路径，该树索引了四字节整数。树的高度是4.出于说明目的，样本路径以这样的方式构造，即所有四种节点类型都出现在其中。搜索树中的路径是为整数搜索关键字218237439构造的，它由四个单字节部分13&2＆9＆255组成。根节点恰好是Node4类型。在前往叶子的路上，遇到其他三种节点类型。   
![](/images/Figure4_12.png)    
&emsp; 在设计节点类型时，重点是存储效率和搜索效率。例如, 节点类型 node16 的构造使具有矢量指令的现代处理器上的所有存储的搜索键 (即所有16个字节) 都可以并行比较。  
&emsp; 除节点适应性外，ART中还包含两个进一步的优化：由于不需要扇出, 导致单叶的长搜索键被折叠。因此, 这些叶子被拉到搜索树的内部。插入另一个具有相同前缀的键之后, 这些节点将在生成的树中沉入较低的位置。 此外，节点的所有搜索关键字共有的前缀被“分解”并仅存储一次。这对于索引键 (如所有前缀为 "http://" 的 url) 特别有用。这些技术与自适应节点大小调整相结合，可确保每个键的最坏情况空间使用受52个字节的约束。   
####  4.3.4并发控制  
&emsp; 由于HyPer是混合OLTP / OLAP引擎，因此保持快速扫描性能至关重要。因此，在设计用于事务隔离的多版本并发控制方案时，必须安装最新的更新以保留连续的数据放置，以便处理器的预取器可以在扫描关系时主动将数据移动到缓存中。MVCC主要用于隔离并行OLTP事务。仍然可以为计算密集型分析查询分叉OLAP快照。  
&emsp; 图4.13说明了使用传统银行示例的版本维护。为简单起见，数据库由一个Accounts表组成，该表仅包含两个属性：Owner和Balance。HyPer不会创建新版本; 并在更新事务的撤消缓冲区中维护更新（但未提交）和替换版本之间的后向增量。就地更新数据保留了数据向量的连续性, 这对于高扫描性能至关重要。    
&emsp;&emsp;&emsp;&emsp;![](/images/Figure4_13.png)   
&emsp; 在提交事务时，新生成的版本增量必须重新生成以确定其有效间隔。在其撤消缓冲区中对事务的所有版本增量进行聚类可以极大地加速此提交处理。此外，使用撤消缓冲区进行版本维护，MVCC模型几乎不会产生存储开销，因为它需要在事务处理期间维持版本增量（即，更改的前映像），无论如何都要进行事务回滚。唯一的区别是撤销缓冲区（可能）维持稍长的持续时间，即，活动事务可能仍然需要访问撤消缓冲区中包含的版本。因此，图4.13中所示的VersionVector锚定了“最新到最旧”方向的版本重建增量链（即列值），可能跨越不同事务的撤消缓冲区。即使对于列存储后端，每个记录也只有一个VersionVector，因此版本链通常连接一个记录的不同列的前映像。  
&emsp; 只有一小部分数据库会被版本化，因为旧版本在不再需要时会不断地进行垃圾回收。如果所有活动事务都在增量标记后开始, 则版本 (重建增量) 将过时。每当相应的记录未版本化时，VersionVector都包含null，否则返回指向撤消缓冲区中最近替换版本的指针。    
&emsp; 在示例中，只有两种事务类型：转账交易标记为“来自(from!to)！ 通过首先从一个帐户的Bal中减去1然后将1添加到另一个帐户的Bal，将$ 1从一个帐户转移到另一个帐户。只读事务表示所有余额的总和, 在我们的 "封闭世界" 示例中, 应始终计算150美元, 无论它们在什么情况下运行时间戳。  
&emsp; 进入系统的所有新事务都与两个时间戳相关联：transactionID和startTime-stamps。提交后, 更新事务将收到第三个时间戳, 即确定其序列化顺序的提交时间戳。最初，所有事务都分配了高于任何事务的任何startTime-stamp的标识符。  
&emsp; 更新事务就地修改数据，在撤消缓冲区中保留旧版本的数据。这个旧版本有两个目的：（1）在事务被回滚（撤消）时需要它作为前映像，而（2）它作为到目前为止有效的已提交版本。当更新程序仍在运行时, 新创建的版本将使用其事务 id 进行标记, 因此未提交的版本只能由更新事务本身访问。在提交时，更新事务接收其commitTime-stamp，其中的版本增量（撤消日志）被标记为与从“现在”开始的事务无关。此commitTime-stamp取自生成startTime-stamps的同一序列计数器。    
&emsp; 在该示例中，在时间戳T3（Sally！Wendy）处提交的第一个更新事务在其撤消缓冲区中创建的版本增量分别为Sally和Wendy的余额加上时间戳T3。时间戳指示必须对startTime低于T3的事务应用这些版本增量，并且对于从T3开始的事务，后续版本对此有效。在startTime T4，具有transactionID Tx的读取器事务进入系统并仍处于活动状态。它将读到的Sally的余额为重建值9，Henry的重建值为10，Wendy的重建值为11.另一个更新事务（Sally！Henry）在时间戳T5提交。同样，属于Sally和Wendy的余额的版本在T5更新之前有效，在T5的撤消缓冲区中保留为之前的映像。请注意，重建版本从其前任的时间戳到其自己的时间戳有效。因此，使用T5的撤消缓冲区重建的Sally的Balance版本从时间戳T3到时间戳T5有效。如果版本增量没有前导（由空指针指示），例如在T5的撤销缓冲区中的Henry余额版本，则其有效性被确定为从虚拟时间戳“0”直到时间戳T5。具有低于T5的startTime的事务的任何读取访问都应用此版本增量，并且任何具有高于或等于T5的startTime的读取访问将忽略它，从而读取Accounts表中的就地版本。    
&emsp; 尚未提交的版本的增量接收临时时间戳，该时间戳超过已提交事务的任何“实际”时间戳。    
&emsp; 这是为更新事务（Sally！Henry）举例说明的，该事务被分配了更新程序的transactionID时间戳Ty。这个临时的，非常大的时间戳最初被分配给Ty的撤消缓冲区中的Sally的Balance版本增量。任何读取访问，除了事务Ty的那些，具有高于T5的开始时间戳（并且显然低于Ty）将应用此版本增量以获得值8。只有Ty可以看到Sally平衡值为7的未提交的就地版本。   
##### 可序列化验证  
&emsp; HyPer的MVCC方法故意避免写写冲突，因为它们可能导致级联回滚。如果另一个事务尝试更新未提交的数据对象，则会中止并重新启动它。因此，第一个VersionVector指针始终指向包含已提交版本的撤消缓冲区 - 对于指针为空的无版本记录。如果同一事务多次修改同一数据对象，在同一个撤消缓冲区内有一个内部指针链，最终会导致提交的版本。   
&emsp; 为了保留可扩展的无锁系统，HyPer依赖于其MVCC模型中的乐观执行。在没有任何进一步验证的情况下，所描述的CC方案保证（仅/已）快照隔离。为了保证完全可串行化，在事务结束时需要验证阶段，以确保在事务处理期间的所有读取都可以在事务的最后读取（逻辑上），而不会发生任何可观察到的更改。验证检测到四个相关的转换: 与事务 "真正" 相关的对象T的修改、删除、创建、创建和删除,为此，事务从计数器中绘制一个commitTime-stamp，它也生成startTime-stamps。新绘制的数字决定事务的序列化顺序。只有在T的生命周期内，即在startTime和commitTime之间提交的修改才相关，如果这些修改/删除/创建的对象确实与T的读取谓词空间相交。   
&emsp; 与之前可能需要重新检查大型读取集（和扫描集）的验证方案相反，HyPer将验证限制为最近更改和提交的数据对象的数量，无论事务的读取集有多大。为此，HyPer使用一种称为精确锁定[69]的旧（并且在很大程度上被“遗忘”）技术，它消除了谓词锁定的固有可满足性测试问题。这种精确锁定测试的这种变化针对正在验证的事务的面向谓词的读取对最近提交的事务进行离散写入 (更新、删除和记录插入)。因此，如果这种扩展写入与验证中的事务的内涵读取相交，则验证失败[155]。验证如图4.14所示，其中事务T在三个不同的谓词P1，P2和P3下读取对象，形成T的谓词空间。我们需要在右侧验证三个撤消缓冲区，并验证它们的对象（即数据点）不与T的谓词相交。这是通过评估这些对象的谓词来完成的。 如果谓词不匹配，则没有交集并且验证通过，否则存在冲突。   
&emsp;&emsp;&emsp;&emsp;![](/images/Figure4_14.png)   
&emsp; 为了找到在事务T的生存期内提交的其他事务的扩展写入，HyPer维护一个recentCommitted事务列表，其中包含指向相应撤消缓冲区的指针。验证从T的startTime之后提交的最旧事务的撤消缓冲区开始，并遍历最小的事务（列表底部的图4.13）。每个撤消缓冲区的检查如下：对于每个新创建的版本，验证检查它是否满足T的任何选择谓词。如果是这种情况,T的读取集是不一致的, 因为检测到的幻象, 它必须中止。对于修改（更新），映像前后都要被验证。如果要么与T的谓词空间相交，那么它将被中止。图4.14描述了这种情况，其中最低撤消缓冲区的数据点x满足谓词P3，这意味着它与T的谓词空间相交。   
&emsp; 成功验证后, 事务T是通过首先将其提交写入重做日志 (这是持久性所必需的) 来提交的。此后，所有T的transactionID时间戳都更改为新分配的commitTime-stamp。由于撤消缓冲区中的版本维护，所有这些更改都是本地的，因此非常便宜。如果由于验证失败而导致中止，则会发生通常的撤消回滚，这也会从版本链中删除版本增量。请注意，MVCC模型中的可串行化验证可以由几个事务并行执行，这些事务的序列化顺序已由commitTime-stamps确定。   
#### 4.3.5查询处理  
&emsp; 表达式脚本脚本语言是将应用程序逻辑（作为存储过程）直接推送到数据库系统而不是依赖于应用程序服务器的关键。为此，HyPer使用HyPer-Script语言将声明性SQL与命令式构造（如循环和分支）集成在一起。例如，我们使用TPC-C基准的newOrder过程的框架。此过程插入一个新的客户订单，该订单由作为表值参数位置传递的可变数量的订单位置组成。   
create procedure newOrder (w_id integer not null, ...,  
&emsp;&emsp;&emsp;&emsp;&emsp;table positions(line_number integer not null,  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;supware integer not null,   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;itemid integer not null,   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;qty integer not null),  
&emsp;&emsp;&emsp;&emsp;&emsp;datetime timestamp not null) // TABLE-valued parameter above     
&emsp;&emsp;&emsp;&emsp;&emsp;{   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; select w_tax from warehouse w where w.w_id=w_id;   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;... // w_tax value used later      
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;insert into orderline // insert all the order positions   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;select o_id,d_id,w_id,line_number,itemid,supware,null,qty,  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;qty*i_price*(1.0+w_tax+d_tax)*(1.0-c_discount),   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;...  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;from positions, item, stock  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;where itemid=i_id and s_w_id=supware and s_i_id=itemid  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;returning count(*) as inserted; // how many were inserted?  

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;if (inserted<cnt) rollback; // not all=>invalid item=>abort      
&emsp;&emsp;&emsp;&emsp;&emsp;};   
&emsp; Hyperscript 允许 "正常" SQL查询, 其结果将在程序后面使用.第一个查询就是一个例子, 该查询选择了特定仓库的税率, 然后在将订单位置插入关系订单行时使用此变量w_tax。此示例脚本首先使用SQL查询从基础表中提取相关信息。然后创建新的订单记录，对该记录的引用也插入到neworder表中。然后更新库存表以记录此新订单。在最后的步骤中，将在表参数位置中传递的各个订单位置插入到订单行表中。为此，计算汇总价格，包括税收，并根据折扣减少。最后，脚本测试是否所有订单位置都已成功插入。如果不是，则必须回滚整个事务（即newOrder脚本）。   
&emsp; 使用声明性脚本语言有许多优点：     
1. 分析声明性脚本以检测安全性问题更容易。 如果存储过程在与实际数据库服务器相同的进程中运行，则这是至关重要的。  
2. 嵌入式SQL查询可以通过常规查询优化器进行优化。 HyPer还使用为查询开发的相同编译技术 - 如下一节所述。  
3. 生成的脚本非常简洁，因此，我们的示例应用程序演示了非常简单的可读性。  

#####  查询和事务的编译  
&emsp; 如第3章所述，HyPer偏离了解释处理模型，并将整个（逻辑优化的）查询计划编译为机器级代码。除了传统的基于迭代器的执行模型之外，还为整个管道生成HyPer查询评估代码。为此目的，逻辑优化的代数树被分段成其最大可能的管道，即两个管道断路器之间的所有代数运算。这是评估以下连接查询的查询计划的示例。   
&emsp;&emsp;&emsp;select *  
&emsp;&emsp;&emsp;from R, S, T  
&emsp;&emsp;&emsp;where T.x=7 and S.y=3 and R.z>5 and   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;T.B=S.B and S.A=R.A   
&emsp; 就关系代数而言，它构成以下三向连接和先前（下推）选择：  
&emsp;&emsp;&emsp;&emsp;&emsp;![](/images/Figure_gongsi.png)  

&emsp;&emsp;&emsp;&emsp;&emsp;![](/images/Figure4_15.png)      
&emsp; 逻辑优化可能导致代数计划如图4.15所示。图4.16显示了生成的代码 - 为简单起见，这是伪代码而不是LLVM代码。      
&emsp;&emsp;&emsp;&emsp;&emsp;![](/images/Figure4_16.png)      
&emsp; HyPer编译器通过调用与每个支持的运算符关联的生产/消费接口函数以模块化方式构造。与解释的迭代器模型相比，这些函数在编译时调用。为了生成以数据为中心的代码，管道运算符直接调用其父运算符的消耗函数。因此，曾经物化的数据对象保留在处理器寄存器中，用于无缝评估一个管道内的所有操作。这在我们的示例伪代码中特别突出，该探测管道以元组r开始，确定选择谓词（z> 5）是否成立，然后探测第一个哈希表连接A以检索一个匹配的元组s 它具有与r（一个接一个）相同的A值，然后探测哈希表连接B，再次检索逐个匹配的t元组并实现组合元组（rst）。  
&emsp; 代数运算符模型对于在查询优化期间推理查询非常有用，但不反映查询在运行时的执行方式。例如，图4.16的第一个代码片段中的三行属于表扫描T，选择t.X = 7，以及分别为连接B构建散列连接表。然而, 查询编译器在逻辑上适用于由查询优化器生成的运算符树, 并静态地将运算符树转换为可执行代码。从概念上讲，每个运算符都提供了一个统一的接口，它与迭代器模型完全不同，但几乎一样简单：它可以按需生成元组，并且可以使用来自子操作符的传入元组。这个概念接口允许生成以数据为中心的代码，同时保留代数运算符模型的可组合性。但请注意，此接口仅是代码生成期间使用的概念 - 它在运行时不存在。也就是说，这些函数仅用于生成适当的LLVM代码以生成和使用元组，但它们不会在运行时调用。   
&emsp;&emsp;&emsp;&emsp;&emsp;![](/images/Figure4_17.png)  
&emsp; 理论上，可以为每个单独的查询生成整个代码。但是，为了减少为后续即时编译生成的代码的大小和复杂性，预先实现运算符的核心（例如，散列连接，索引嵌套循环连接，散列聚合，排序）是有益的。 并且仅动态生成查询相关代码。为了简化实现，此功能通常使用C ++而不是LLVM编写.在这方面，生成的LLVM代码构成了驱动预制代码库的齿轮的链 - 如图4.17所示。
##### 大规模并行查询处理：连接   
&emsp; HyPer查询引擎通过运算符内的并行性充分利用现代处理器的多核计算能力。本节通过描述最重要的运算符之一的策略，总结了HyPer在该领域的创新：连接。在不久的将来，数据库服务器将拥有数百个计算核心。他要求并行处理任务必须尽可能自主，没有同步点，这可能导致许多核心的空闲等待时间。核心越多，阿姆达尔定律的影响越严重。由于具有大DRAM容量的多核服务器的非均匀存储器访问（NUMA）特性，仅仅并行化计算是不够的。在本地分配计算任务也很重要  
&emsp; **大规模并行排序/合并-连接。** HyPer的大规模并行排序合并（MPSM）连接旨在考虑NUMA架构，而这些架构在主内存系统的并行连接处理方面尚未成为以前工作的重点。MPSM依赖于将参数分块为与可用于并行处理的核心一样多的相同大小的片段。与传统的排序合并连接不同，MPSM避免合并排序的运行以获得全局排序顺序，而是以蛮力但高度并行的方式将它们全部连接起来，选择在扫描中投入更多资金以避免硬拓扑化合并相。显然，该决定不会导致全局排序的连接输出，而是呈现部分排序顺序，仍然允许基于排序顺序的后续操作，例如，早期聚合。在随后的连接阶段，跨NUMA分区的数据访问是连续的，因此预取器主要隐藏访问开销。 MPSM的细节可以在[9]中找到。   
&emsp; **并行Radix-Hash-Join。** 排序/合并连接使用排序通过（几乎）线性同步扫描有效地连接对象。不幸的是, 排序的成本仍然很高。在equi-joins的情况下，哈希连接的想法非常简单。其中一个连接参数关系被插入到一个哈希表（称为构建输入）中，另一个关系（称为探测输入）被顺序扫描并探测到该哈希表中以查找匹配的连接伙伴。   
&emsp; 将此哈希连接算法并行化的一种可能性是在实际哈希连接计算之前对参数关系进行分区。这里，基数分区显然是一种非常有效的方式，因为它允许在没有昂贵的价值比较和分支的情况下对关系进行分区。分区后，线程可以为其分区独立构建哈希表。   
&emsp;  很明显, 在生成过程中以及在探测阶段, 哈希表的缓存局部性都很低。基本上，可以预期每次访问都会引发缓存故障。因此，将参数关系划分为较小的片段是有益的，这样得到的哈希表适合处理器的缓存，以避免这些昂贵的缓存未命中。但是，将“一次性”关系划分为太多分区对于性能也是有害的，因为复制到片段会导致过多的缓存未命中 -特别地，转换后备缓冲器（TLB）变得太小而不能保存所有写入位置的所有页表条目。因此，可以使用多步分区。   
&emsp; **没有分区的并行哈希连接。** 基数连接会产生相对较高的复制成本，希望在构建和探测哈希表期间通过较高的缓存局部性来分摊。在任何情况下，基数连接都会产生额外的存储成本以维护分区。因此，在内存设置中，将（通常更大的）探测输入保留在原位并仅将（较小的）构建参数关系复制到哈希表中是一个简单的想法。在此过程中，构建阶段需要特别注意，因为许多工作者将数据并行插入此哈希表。这需要哈希表桶上的短期锁存器，工作人员将新数据项插入其中。这些所谓的锁存器可以通过有效的比较和交换机器代码语句来实现。使用此指令设置的锁存器应直接与散列桶关联，以保证它们存储在同一高速缓存行中。   
&emsp; 在构建哈希表之后，可以并行执行探测阶段而没有任何同步开销，因为工作者只从哈希表中读取。每个工作者都在探测参数的一个（最好是NUMAlocal）块上工作，并确定哈希表中这些元组的连接伙伴。显然，如果其中之一的参数关系（即构建输入）（大大地）小于另一个探测参数，则该连接方法特别有效。此外，这种简单的连接方法对管道并行性特别有用[76]  - 如图4.18所示，其中探测管道覆盖了两个哈希表探测器。请注意，对于基数连接，跨多个连接哈希表的流水线操作是不可能的，因为每个二进制连接都需要单独的分区。   
&emsp;&emsp;&emsp;&emsp;&emsp;![](/images/Figure4_18.png)
##### 自适应Morsel-Wise并行化和工作负载管理。  
&emsp; 如今，硬件性能改进的主要推动力来自于增加多核并行性，而不是加速单线程性能。对于具有数十或（很快）数百个内核的架构，我们使用术语“多核”。   
&emsp;  在内存数据库系统中，查询处理不再受I/O限制，并且可以真正利用多核的巨大并行计算资源。不幸的是，将存储器控制器移动到芯片中的趋势以及因此存储器访问的分散导致非均匀存储器访问（NUMA）。从本质上讲, 计算机本身已成为一个网络, 因为数据项的访问成本因数据和访问线程所在的芯片而异。因此，多核并行化需要考虑RAM和缓存层次结构。特别是，必须仔细考虑RAM的NUMA划分，以确保线程（大多数）在NUMA本地数据上工作。
&emsp; 为了应对这些挑战，开发了自适应的morsel驱动的查询执行框架[83]来控制HyPer的并行运算符。对于三向连接查询R连接S连接T，该方法如图4.18所示。 通过并行处理不同核心上的每个管道来实现并行性，如图中的两个（上/红色和下/蓝色）管道所示。核心思想是一种调度机制（“调度程序”），它允许灵活的并行执行操作员管道，即使在查询执行期间也可以改变并行度。查询被分成段，每个执行段占用一小部分（通常为100,000）输入元组并执行这些，在下一个管道断路器中实现结果。morsel-framework允许NUMA本地处理，如图中的颜色编码所示：线程在NUMA本地输入上运行，并将其结果写入NUMA本地存储区域。调度程序运行固定的、依赖于计算机的线程数, 这样, 如果新查询到达, 就没有资源则没有资源超额预订。这些线程被固定到内核, 因此不会因为操作系统将线程移动到不同的内核而意外丢失NUMA局部性。  
&emsp; morsel驱动调度的关键特征是任务分配在运行时完成，因此具有完全的弹性。即使面对中间结果的不确定大小分布，这也可以实现完美的负载平衡，以及现代CPU内核难以预测的性能，即使它们获得的工作量相同，它们也会有所不同。它具有弹性，因为它可以处理在运行时更改的工作负载（通过减少或增加已在执行的查询的并行性），并且可以轻松集成一个机制以运行不同优先级的查询。
&emsp; 由于只是调度到完整的查询执行框架，所有物理查询运算符必须能够在所有执行阶段（例如，散列构建和探测）并行执行，这是一个至关重要的需求。 根据Amdahl定律实现多核可扩展性。morsel-driven框架的一个重要部分是对数据局部性的认识。这从输入morsels和物化输出缓冲区的局部性开始，但扩展到可能由运算符创建和访问的状态（数据结构，如哈希表）。此状态是可由任何内核访问的共享数据, 但具有高度的NUMA局部性。通过主要访问本地RAM，优化了内存延迟，并且最小化了跨套接字内存流量（可以减慢其他线程的速度）。      
#### 4.3.6持久性和恢复   
&emsp; 对于原子性，HyPer会写一个无需写入稳定存储的撤消日志。它仅作为DRAM中的环形缓冲区维护，因为一旦事务完成，它的条目就可以安全地被覆盖。事务的持久性要求在失败后必须还原已提交事务的所有影响。为了实现这一目标，HyPer采用了经典的重做日志。图4.19 中导致非易失性重做日志存储设备的事务流中产生的,灰色椭圆突出显示了这一点。最初，HyPer通过记录表示事务的存储过程的参数来使用逻辑重做日志记录。在传统的数据库系统中，逻辑日志记录存在问题，因为在系统崩溃后，数据库可能处于动作不一致状态。这在HyPer中不会发生，因为它从事务一致性存档重新启动（参见图4.19）。然而，逻辑日志记录证明是有问题的，因为它需要完全确定的事务，这在实践中很难实现。特别是，中止的事务可能会改变在SQL语句中检索元组的顺序。因此，HyPer切换到物理日志记录，这会导致更高的日志量，因为通过将相应的后映像值推入日志流来记录所有更新。   
&emsp;&emsp;&emsp;&emsp;&emsp;![](/images/Figure4_19.png)   
&emsp; HyPer还可以利用VM快照在非易失性存储上创建整个数据库的备份存档。该过程如图4.19所示。 通常，存档通过10到100 Gb / s的高带宽网络或甚至通过Infiniband集群中的RDMA写入同一计算中心内的专用存储服务器。为了保持这种传输速度，存储服务器必须使用几个（大约10个）磁盘来获得相应的聚合带宽。   
&emsp; 在HyPer的ScyPer横向扩展中，重做日志用于在主服务器发生故障时将辅助服务器“馈送”为备用OLTP处理器。VM快照机制允许在辅助服务器上为OLAP处理分叉一致的快照。从而在主服务器和辅助服务器之间进行负载平衡分析查询处理。  
#### 4.3.7进一步说明  
&emsp; 包括虚拟内存快照在内的HyPer架构于2011年首次推出[72]。在[104]中分析了基于虚拟内存管理与软件控制机制的快照的优点和性能。查询和事务脚本的开创性JIT编译是在[109]中开发的。Pirk等[123]分析了跨列存储和行存储之间的整个设计空间的混合存储表示的影响。虽然列存储擅长分析, 行存储最适合事务, 但处理最佳表示形式取决于工作负载。  
&emsp; 在一系列论文[106,132]中描述了HyPer向群集中多个节点的横向扩展。对于非常快速的Infiniband网络基础设施，Roediger等[131]分析了查询引擎的基于RDMA的专用通信协议。Muhe等人分析了HyPer在多租户应用中的使用[105]。HyPer占用小，可为每个租户分配一个专用的HyPer实例;从而实现了不同租户数据的完全分离, 从安全角度来看是有益的。  
&emsp; 大规模并行排序 - 合并连接MPSM由[9]开发。[76]描述了依赖于全局哈希表的管道哈希连接，如[22]所提出的。如果可以在一个管道中执行多个连接或者当其中一个连接参数小于另一个时，与[15]中分析的基数分区连接相比，它特别有用。基于这些算法，在[83]中设计了HyPer查询引擎的全面并行化。 自适应基数树ART是在[85]中设计的。   
&emsp; HyPer具有先进的SQL窗口功能，可提供强大的决策支持功能; Leis等人展示了如何在多核服务器上有效地并行化这些窗口函数[87]。HyPer具有[110]中描述的高级查询取消方法。与[84]中其他市场领先的系统相比，对HyPer的整体查询优化器进行了评估。[111]中描述了多版本并发控制。如[86]中所述，英特尔Haswell处理器的最新硬件事务存储器功能支持数据结构的同步。 

### 4.4 SAP HANA
#### 4.4.1 引言
SAP中的内存处理技术是在HANA诞生之前的几年开始进行的。SAP HANA最早的直接前身是TREX文本搜索引擎（始于2001年），它的存储文档的元数据属性在主内存中呈柱状格式。但是在那个时候，将这些数据集保存在内存中与否并不重要，因为文档属性的内存占用量很小。从2005年便开始开发BWA，即SAP 商业数据仓库加速器。使用SAP BW部署的选定数据多维数据集可以从传统的基于磁盘的数据库系统复制到BWA中，以进行快速查询处理，尤其是加速典型的OLAP样式查询（例如，沿维层次结构的聚合操作）。早在2005年，内存成本与性能的价格 - 效益关系对于许多客户来说都是可以接受的，而BWA在SAP BW客户中变得非常受欢迎。接下来SAP内不同产品组内联合，并采用逐步演进方法启动SAP HANA，从支持扩展SAP BW安装的数据集市场景到添加更传统的数据库功能：基于TREX / BWA的技术被用作柱存储引擎的基础。P * TIME [28] SAP在2005年开发的内存数据库解的决方案贡献了行存储引擎，SQL子系统以及连接管理等技术的发展。 MaxDB引擎主要贡献了持久层以及周围的基础架构/工具的开发。P * TIME [28] SAP在2005年开发的内存数据库解决方案促进了行存储引擎，SQL子系统以及连接管理的发展。 MaxDB引擎主要促进了了持久层以及周围的基础架构/工具的发展。

Hasso Plattner开发的愿景不仅是拥有一个集成数据管理平台的单一系统，而且还希望完全依靠内存技术来发掘现代硬件的功能。在现代业务应用程序需求的驱动下，紧密结合运营和战术战略业务决策等方面，总体目标是满足OLTP和OLAP工作负载，打破运营ERP和仅用于支持决策过程的传统DWH系统之间的隔阂。回顾过去，将重点放在内存技术上的决定是正确的：一个标准的ERP部署展示了一个大约5 Tbyte的未压缩数据库卷，其大小由商品服务器硬件提供。使用SAP HANA的压缩方案（请参见第4.4.2节），即使是最大的100 TB的SAP BW安装也可以满足合理的硬件方案。使用动态分层方法实现与SAP Sybase数据管理基础架构（从数据流引擎（SAP Sybase ESP）到SAP Sybase IQ）的紧密集成，HANA支持企业级的大数据需求，并为满足现代商业应用以及在相同数据集上工作的数据科学家提供了坚实的基础[98 ]。

##### OLTP和OLAP的组合
自SAP HANA项目的早期开始，其为客户做出的主要设计之一便是允许在单个数据库管理系统的环境中事务的统一性和复杂的工作负载分析[124]。虽然乍一看这与不能满足所有要求的现实相矛盾，但是仔细观察就会发现系统在几个场景中的某一方面表现得相当好（例如维持大型企业资源管理（ERP）系统的OLTP工作负载[78]）并且在另一个方面（例如，非常快速的OLAP性能）表现出色，可以为最终用户带来显著的好处。

为了处理需要非常高吞吐量的OLTP方案，SAP HANA采用了基于P * TIME系统构建的内存行存储[28]。 在抽象层面上，行存储与第4.1章中讨论的Hekaton系统共享几个关键概念和设计选择方案（例如无锁索引）。 但是，为了避免行和列式格式的冗余数据存储，SAP HANA列存储的目标是使大规模OLTP操作执行顺畅，并且在允许对新数据进行复杂分析操作方面表现优异，而没有任何传播延迟或数据冗余。

对于SAP HANA中的列式数据存储，面临的挑战是以允许处理OLTP系统的特征工作负载的方式设计系统（例如，基于主键的查找操作，具有高选择性的键或外键连接，原位更新操作，完整记录检索），而不会影响OLAP性能[138]。 为此，引入了几个OLTP优化：
1. **生成查询计划**：在最初的设计中，查询编译器没有使用为SAP HANA列存储准备语句的概念。虽然此概念适用于复杂的OLAP工作负载，但OLTP查询通常非常简单。 因此，查询编译开销（几毫秒）就非常昂贵，因此要求使用预准备语句和计划OLTP的快捷方式
2. **并行有害化**：SAP HANA中的实时系统和查询处理运算符在所有可能的级别上大量使用并行化，如第4.4.5节中所述。 虽然这对于复杂分析查询实现良好的报告性能非常有益，但并行化可能被认为对OLTP操作有害：例如，对启用了并行化的非常小的结果集执行简单的键/外键连接操作，可以轻松增加运行时 由于作业调度和上下文切换的高开销，因此减少了两倍。 因此，SAP HANA中的查询编译器和运行时系统在根据查询复杂性和系统工作负载决定正确的并行化策略方面付出了很多努力。
3. **并发OLTP和OLAP调度**：本节中讨论的各种设计和优化使SAP HANA中的列式存储能够处理大容量OLTP工作负载。虽然由于事务工作负载的列式存储和字典压缩的缺点，系统不能胜任此任务，但此方法可以执行复杂的分析查询，而无需数据冗余，数据复制或其他调整（例如，重型索引或物化视图的使用）。但是，需要额外注意确保复杂的分析查询不会影响（大量）并发OLTP操作：阻止它们（例如，通过大量并行化OLAP查询导致资源不可用）可能很容易导致交易工作量下降并对整个系统产生负面影响。因此，HANA中的工作负载管理组件确保OLTP操作永远不会因过于仔细的工作负载监控和基于类的查询调度而停滞，即事务操作是优先于OLAP查询的高优先级查询类[125]。

#### 4.4.2 数据组织
SAP HANA的内存数据表示旨在满足两个基本要求：复杂的分析查询的处理性能需要最大化，同时应最小化内存占用，以便合理处理大量硬件数据量。

**字典压缩**：以列式格式存储的所有数据都使用字典压缩。除了减少内存占用外，字典压缩还可以实现高效的扫描操作，如第4.4.5节所述。此外，密集域编码也可以在系统的其他部分中使用，并且通常提供有趣且非显而易见的好处（例如，用于保持非常紧凑的直方图[101]）。因此，存储在SAP HANA中的每个列都包含图4.20中描述的两个核心数据结构，其中字典存储特定列中出现的所有不同值，索引向量指示哪个值存储在此列的特定行中。虽然图4.20描述了位置以及对应的值/ valueID，但这些位置纯粹是虚拟的，可以根据数据结构中的偏移量进行计算。每个列都可以选择使用索引进行扩展，以便于处理高度选择性的查询（例如，第4.4.1节中讨论的OLTP工作负载）。索引利用字典压缩并实现为反向列表，将valueID映射到相应的行号。

**增量和主体的概念**：为了获得最大的查询处理性能，SAP HANA会对字典进行排序。这使运行时系统能够直接对字典编码的值执行比较（例如，每当a<b包含一组特定的值时，相应的valueID也同样如此）。虽然此属性对范围查询非常有益，但它对更新处理带来了额外的挑战：在向字典添加新文字时，字典中后续条目的valueID需要增加，这可能导致索引向量中需要更新的大量条目。通常，这使得更新操作的成本非常昂贵。为了避免这个问题，HANA中的所有列表都分为两部分，即主表和增量表。

列的主要部分由排序字典和索引向量组成。表中的所有插入操作都由增量部分处理，该部分由索引向量（类似于主索引）、未排序字典（以避免上述重新编码问题）以及基于树的索引结构组成，以提供对字典的快速访问。每当一个新条目插入到增量中时，索引用于检查相应的文本是否已经存储在字典中。如果是这样，将检索相应的valueID，否则将在字典结构的尾部和树索引中添加一个新条目。更新操作被建模为一系列删除和插入操作。需要删除的主体的行由单独的位向量跟踪。

虽然表的增量部分避免在更新时对索引向量的大部分进行重新编码，但是查询处理必须参考这两种数据结构。此外，增量上大多数核心数据访问原语的效率不如主体部分。另外，它的附加索引结构会消耗内存，从而与高数据压缩的目标相冲突。为了避免这些缺点，系统在一个称为delta merge的操作中定期将增量合并到主体中。根据系统根据成本函数自动做出的基于成本的决策执行增量合并的最佳时间点，该成本函数（除其他功能外）考虑增量和内存（根据内存、记录数和磁盘占用）的大小以及当前系统工作负荷。

![figure4-20](./images/figure4-20.PNG)
##### 压缩技术
为了将压缩开销保持在最低水平，SAP HANA仅使用轻量级压缩技术，允许高效地访问单个值和值块。以下所有技术的基础是域编码，它独立地应用于每一列[107]。对于域编码，一个列的n个不同值中的每一个都被分配一个介于0和n之间的整数代码，并且该列的每个值都被存储在字典中的代码字替换。然后，代码字序列（索引向量）存储为位流，其中每个代码字只使用固定数量的b=log(n)位。然后，使用以下几种技术之一对该索引向量进行进一步压缩.

**前缀编码**：这是最简单的压缩技术，主要是删除列开头相同值的重复，并替换为一个值及其频率。如果最频繁的值不仅出现在前缀中，而且还分散在其他值之间，那么可以通过稀疏编码实现良好的压缩。对于稀疏编码，删除出现最频繁的值，并在应用前缀编码的情况下将其位置存储在位向量中。

**聚类编码**：使用这种技术，数据被阻塞，只有具有单个不同值的块通过只存储单个值进行压缩。此外，还需要一个位向量来指示压缩了哪些块，以便能够重构原始列。如果数据块包含多个但仍然只有几个不同的值，则可以使用间接编码。

**间接编码**：在此过程中，域编码应用于合适的块，这为每个块添加了需要单独的最小目录的间接性。 为了减少字典的数量并因此减少存储器消耗，只要字典中的任何新条目不增加编码条目所需的位数，就可以将一个字典用于连续的块。 仅当字典和引用占用的空间小于原始（域编码）数据时，才会压缩块。

**游程编码**:此处介绍的最终压缩技术是运行长度编码的略微修改变体，它将重复值的运行压缩为每次运行的单个值以及运行中的重复次数。 先前值的累积频率总和产生运行中每个值的起始位置。 由于这会带来很大的开销，SAP HANA通过仅存储起始位置而不是重复次数来略微减少压缩。

**列排序问题**:为了最大限度地压缩，所提出的技术倾向于使用相同值的长范围作为排序表示的结果。遗憾的是，列不能彼此独立地排序，因为值的位置反映了其行在表中的位置。将值位置与行位置分离将需要昂贵的值到行的映射的开销。具有相同值的代码字序列称为余数段。列排序问题在于识别最佳全局排序顺序以实现最大可能的剩余段。由于解空间随着列数呈指数增长，因此实际上不可能找到列排序问题的最佳解决方案。因此，SAP HANA部署了几种贪婪的启发式方法，它们代表了缩短运行时间和减少内存消耗之间的折衷。最简单的启发式方法将每列中最常用的值排序到顶部，同时考虑列之间的依赖关系。使用更高级的方法，可以进一步对剩余段进行排序，而不会中断现有的排序。

##### 数据老化
业务数据通常在其生命周期的开始时经常访问，此时相应的业务流程仍处于活动状态。 经过一段时间后，根据其状态，在正常操作期间不再将数据作为常规工作集的一部分进行访问。 但是，旧数据可能再次变得有用，例如在多年运行分析时，访问客户的订单历史记录时或在审计期间。 这种观察通常以数据温度隐喻为特征。 正在操作相关的数据称为热数据，在正常操作期间不再访问的数据称为冷数据。  数据温度可用于水平分区应用程序数据，以优化资源消耗和性能。 在不同分区之间（即从当前分区到历史分区）移动数据的过程称为数据老化。

老化的目标是通过仅在主存储器中保持操作相关的数据来减少主存储器占用并加速数据库查询。 历史数据可以以不同方式存储，加载和访问，但仍可通过SQL访问。 老化概念必须与应用程序的需求和细节相匹配：应用程序以不同方式存储数据，具有不同的表方案，并具有不同的访问模式。 对于SAP HANA上的BW，老化几乎是透明的。 BW使用作为主键列的时间维度按范围对数据进行分区，使老化成为物理数据库设计的固有部分。 用户请求较少的旧数据，因此相应的分区通常驻留在磁盘上。

更一般地，业务对象通常跨越多个表，例如header和lineitem表。 它们具有状态（如订单执行），但这种状态可能只在其中一个表中设置。 因此，使用状态作为分区列通常既不充分也不总是可行的，因为分区始终与存储元组的单个表的列相关。

为了维护构成业务对象表并向后兼容应用程序，SAP HANA为老化感知表保留了一个人工温度列。 应用程序主动将此列中的值设置为日期，以指示对象已关闭，并且该行应移至冷分区。 它对同一业务对象的所有相关表一致，甚至对一组相关业务对象也是如此。

从应用程序的角度来看，对hot的默认限制意味着冷存储中的行不再存在，就像它们被删除一样。 因此，应用程序必须与老化标准保持一致，并最终注意将哪些行或业务对象移动到冷分区，以便始终提供正确的结果集。 对于数据访问，这意味着当以两种方式之一读取数据时，数据始终是一致的，无论是仅热还是冷，具有给定日期以及比此日期更新的所有内容，包括热分区。

对于OLTP处理，应用程序通常仅使用谓词来访问热数据。 通用SQL扩展可用于在应用上述语义规则时过滤冷数据。 可以逐页读取冷分区的数据，仅从磁盘读取最小数据集。 这与在处理之前将所有内容读入内存的列的标准加载行为相反。 具有冷数据的页面将加载到应用LRU机制的页池中。

#### 4.4.3 索引
除了在内存中列式存储，SAP HANA还基于P * Time [28]实现了内存中的行存储。在P * Time内，一种称为乐观锁存器自由索引访问协议（OLFIT）的优化协议[27]被用于在多核环境中，从而获得了良好的索引性能和可伸缩性。 OLFIT协议解决了传统索引并发控制方案在多核环境中的显著缺点，在多核环境中，频繁的加锁操作会导致大量缓存一致性丢失，而这仅仅是因为并发控制，即使只有读取操作而没有进行更新。这被认为是限制内存中多处理器系统的索引性能可扩展性的主要因素。 OLFIT协议通过乐观控制方案解决了这个问题，该方案不需要为读操作设置锁存器。相反，只有写操作设置锁存器，并且仅针对那些实际更改的节点。此外，每当更改节点时，都会增加版本号。读取操作使用乐观方法来确保一致性：它们读取锁存器，如果未设置，则它们继续读取读取操作之前和之后的版本号。如果设置了锁存器或者在读取时更改了版本号，则重复读取操作。通过此优化，读取性能几乎与没有并发开销一样高，并且整体索引性能几乎与处理器核心数量成线性关系。

#### 4.4.4 并发控制
SAP HANA使用多版本并发控制（MVCC）（参见[155]获取教科书示例）以确保一致的读取操作。 MVCC由所有SAP HANA数据存储实现，用于支持事务级别快照隔离和语句级别快照隔离。

**一致的视图和事务时间戳**：对于每个数据库读取操作（事务或语句，取决于所需的隔离级别），SAP HANA提供了数据库的一致视图，其中只包含读取操作可以看到的数据。为了确定给定读取操作可以看到哪些数据，可见性信息与每个数据版本一起存储。对于提交的版本，可见性信息基于事务时间戳。为了避免在事务提交的大量数据版本上更新时间戳，使用了一个间接级别，其中事务提交的所有版本都指向包含提交时间戳的公共事务块。通过使用提交时间戳延迟更新数据版本信息，可以随后异步地删除此间接性。尚未提交的版本将使用创建它们的事务的标识符进行标记。

用于MVCC的事务时间戳(TS)是来自提交计数器的整数，由事务管理器维护并在每次成功提交后递增。读取操作的一致视图有一个相关的时间戳(CVTS)，它是读取操作启动时提交计数器的值。如果在这两者之间没有提交事务，那么多个读取操作可能具有相同的CVT。对于给定的一致视图，在一致视图时间戳之后提交的所有数据版本都不可见。只有那些尚未提交的版本是由读取操作自己的事务创建的。

**合并数据版本**：在任何潜在的一致视图中不再可见的旧数据版本会定期合并以释放内存。 版本合并作为常规计划后台作业异步完成，由特定系统事件触发，或由管理员手动完成。 事务管理器维护系统值MinReadTS，这是必须保留的最早的一致视图的时间戳，因为至少有一个活动读取器需要访问它。 每当事务结束时，事务管理器检查这是否是具有等于MinReadTS的一致视图时间戳的最后一个事务。 如果是这种情况，MinReadTS可以前进到活动事务的下一个更大的CVTS。 如果任何旧数据版本具有在与MinReadTS相对应的一致视图中可见的后继版本，即，如果后继版本具有小于MinReadTS的时间戳TS，则可以移除任何旧数据版本。

**写冲突**：MVCC确保一致的读取操作，但不会阻止对同一数据版本的并发写入操作，这可能导致相关的不一致，例如脏写和丢失更新。 为了防止对相同数据版本的并发写入操作，SAP HANA为每个写入访问请求采用独占的行级写入锁定。 事务管理器通过中止受害者事务来执行死锁检测和避免。 使用快照隔离时，如果允许事务创建对其可见的新版本记录，则会发生丢失更新，同时其他事务将提交新版本。 SAP HANA检测到此类写入冲突，并使用序列化错误中止违规操作。

**快照隔离和惟一性约束**：SAP HANA支持唯一性约束（对于主键列和其他列）。 检查违反唯一性约束不是针对写操作所看到的一致视图。 相反，检查包括所有现有版本。 如果写入操作与创建一致视图后提交的版本产生唯一性冲突，则将中止write语句的执行。 如果存在与未提交版本的约束违规冲突，则执行将阻塞，直到另一个事务完成或直到等待写入操作中止并超时。

#### 4.4.5 查询处理
全表扫描的性能对于列存储数据库系统的整体查询性能至关重要。压缩基础列数据格式既是优势又是挑战，因为它一方面减少了扫描中涉及的数据量，另一方面又在扫描期间引入了解压缩的需要。由于SAP HANA中的所有列都是字典编码的，并且对字典条目的引用是位压缩的，因此扫描期间的解压缩速度是一项关键任务。使用矢量引擎（例如，在所有标准英特尔处理器中可用）矢量化扫描允许执行SIMD（单指令多数据）指令以显着加速性能。英特尔AVX2实现向量矢量移位和收集指令，这些指令对SAP HANA核心扫描原语的性能起着至关重要的作用。由于这些SIMD指令适用于机器字的向量，例如8,16,32或64位整数，因此一些有效实现的解包逻辑需要对列值进行解压缩[158]。

对4到256个并行的代码字（具有256位寄存器）执行以下向量化步骤：在第一步中，将数据引入扫描谓词可以处理的格式。在最一般的情况下，这意味着将包含代码字位的所有字节混合到同一机器字中，清除机器字的高（未使用）位，即将它们设置为零，将代码字与机器字边界对齐，即将机器字移到右边，最后将结果存储到缓冲区中。在第二步中，评估谓词。对于范围扫描，这包括两个比较，并且可以在清除之后通过在扫描之前移动范围一次直接完成。矢量化谓词可以在对齐机器字边界后直接进行评估，而其他谓词的评估会被延迟，直到填充缓冲区以便分摊虚函数调用等。在第三步中，必须提取谓词评估的结果，在任意谓词的情况下，是指示匹配的位或索引，还是解压缩的代码字。最后一步是存储提取的结果。
#### 4.4.6 持久性和恢复机制
SAP HANA中的事务持久性是使用日志记录和阴影分页实现的。 事务所做的任何更改都会在逻辑上记录到预写日志中。 SAP HANA还定期写出保存点（a.k.a.检查点或其他数据库系统中的快照）。 因此，为了恢复某个数据库状态，最后一个保存点的内容加上自上一个保存点以来的日志就足够了。 虽然这个概念类似于传统的数据库系统，但完整的持久层是从头开发的，专为分布式内存数据库需求而设计。 以下示例突出了特定的设计决策：

**非传统缓冲区缓存机制**：SAP HANA使用通用资源容器，以统一的方式管理加载的内存中对象（例如数据列和字典，甚至单个数据页）。 该资源容器与存储器管理子系统协作，使得存储器的压力不会导致分页，而是基于加权LRU算法释放（希望）不需要的资源。

**页面大小可变**：该子系统支持多种页面大小（4的幂次，从4KB到16MB），而不是仅支持单个页面大小，以便为连续的柱状数据范围最佳地使用I / O带宽，同时保持持久的空间碎片非常低。 例如，一个20 MB的对象可以由一个16MB和一个4MB页面组成。 这也有助于保持逻辑到物理块转换表所需的存储空间最小化。

**不共享机制**：分布式设置中各个服务的持久状态是完全分离的。此外，除了一致快照之外，各个服务的保存点和日志都是不同步运行的。

**一致性变化**:持久状态的操作被分组为称为一致更改的低级原子操作。 一致的更改包括编写写入UNDO / CLEANUP条目的REDO日志条目，修改持久状态，这可能是对资源容器中加载的页面的实际修改，或者只是通过将某些页面标记为脏并让实现回调实现它们而进行修改的意图 在保存点上。 保证一致的更改要么全部保存在保存点中，要么根本不保留，即更高级别的实现只需要提供REDO操作以重复在线执行的任何操作以创建相同的新逻辑状态。 UNDO / CLEANUP操作由持久性子系统在事务回滚期间（撤消操作）或异步垃圾收集期间自动执行（在任何潜在的基于MVCC的读取器消失后最终删除旧状态）。

除了故障恢复之外，SAP HANA持久性子系统还是多种高级功能的基础，例如基于跨服务的协调一致性快照（完整和增量）的一致在线备份。 这意味着即使日志丢失，分布式数据库也可以从纯数据备份中恢复。 这些子系统还提供时间点恢复以及具有初始数据传输和连续日志传送（接近实时接管）的高可用性解决方案。 通过将页面加密与目标保存点耦合来实现高效安全数据删除的功能是企业数据库解决方案所需功能的另一个示例。

### 4.5 其他系统
这一章的的内存数据库环境不包括我们在前几章中的四个系统。该领域拥有丰富的商业和学术系统，这些系统具有新颖有趣的架构和技术，可以在主存数据上实现高性能。本节总结这些系统：
#### 4.5.1 solidDB
solidDB是一个高性能的数据库系统，最初由Solid Information Technology开发，这是一家私人控股公司，成立于1992年，位于芬兰赫尔辛基。 solidDB于2007年被IBM收购，随后于2014年被出售给UNICOM全球。数据库引擎被继续开发，主要用作电信和网络软件部署中的嵌入式数据库。

在架构上，solidDB是一个混合数据库，由基于磁盘和主内存优化的引擎组成[93]，其中单个SQL语句可以访问来自任一引擎的数据（本摘要主要关注主内存引擎）。 与其他主内存引擎一样，solidDB在访问记录时不使用面向页面的存储来避免间接开销。

solidDB使用Vtrie（用于可变长度trie）索引数据，这是trie数据结构的变体，它索引叶节点，其结构类似于B +树叶节点，其默认大小是几个缓存行。 Vtrie允许读者通过版本化索引节点来进行无争议和无锁存，而编写者使用两级锁定来避免冲突。

对于并发控制，solidDB使用悲观锁[139]; 这与大多数现有系统不同。 对于持久性，solidDB使用快照一致性检查点[91]来恢复到一致状态。 如果需要，用户可以完全禁用事务日志记录以从上一个持久快照中恢复。 solidDB还使用热备份方法提供高可用性，该方法可复制重做日志。

#### 4.5.2 Oracle TimesTen
TimesTen最初是惠普实验室的一个名为Smallbase的研究项目，后来被分拆成一家被Oracle收购的独立公司。 TimesTen继续开发，是Oracle高性能主存数据库解决方案之一。 本概述涵盖了截至2013年Oracle TimesTen的当前功能[74]。

Timesten是一个内存优化的数据库引擎，可用于多种用途，例如独立引擎、Oracle RDBMS上的事务缓存，以及用于交互式BI工作负载的内存存储库。内存部分由内存数据库区域、运行时分配的临时区域和内存日志缓冲区组成。数据库存储避免了页面间接开销，并使用直接指向记录的指针。Timesten还实现了几种索引方法，包括T-Trees支持的哈希、位图和范围索引[79]。Timesten还支持使用基于字典的编码进行列压缩。用户可以指定单独或分组压缩列。

Timesten的一个有趣特性是它的部署灵活性。虽然它可以用作记录数据库，但它很容易部署为基于磁盘的Oracle RDBMS之上的缓存。用户可以指定一组Timesten表作为Oracle数据库中一组对应表的缓存。用户还可以指定如何加载缓存（预加载或动态加载）和同步缓存（只读或可更新）。

对于事务并发控制，Timesten使用无锁多版本控制方法来启用读写并发。它使用行级锁定来处理写并发。提前写入日志以及周期检查点保证了Timesten的耐久性。为了提高性能，Timesten提供了一种延迟的持久性模式，在将提交记录写入内存中的日志缓冲区（而不等待提交记录的持久性）后，确认事务提交。在此模式下，日志刷新每100毫秒发生一次，这定义了可能数据丢失的上限。

#### 4.5.3 Altibase
Altibase是一家位于韩国的内存数据库提供商，成立于1999年。该公司在亚洲市场拥有庞大的客户群，涉及电信，金融和制造公司。 HDB是Altibase的混合数据库系统，包括针对内存驻留数据优化的引擎和类似于传统RDBMS架构的基于磁盘的引擎[11]，其中事务可以跨越两个引擎。 本节的其余部分总结了主存储器引擎的功能。

Altibase将记录存储在页上，其中内存优化表由连续内存中的一个或多个页面组成。 Altibase使用基于页面的组织似乎有两个原因：检查点（检查点是以页面粒度编写的）以及与基于磁盘的引擎的兼容性。 该引擎支持多种索引类型，包括哈希索引，范围索引和空间索引。 索引不以页面格式存储，也不是持久的，并且在恢复期间从头开始重建[11]。

为了支持事务，Altibase内存引擎使用多版本并发控制方法，在每次更新时创建一个新版本（删除创建一个墓碑）。 每条记录都指向其最新版本（最新记录为null），形成一个版本链。 通过使用预写日志记录和模糊检查点提供耐久性。 Altibase在将页面状态复制到事务日志缓冲区时使用页面锁存器，但在将页面数据写入检查点文件时实现无锁定检查点过程[11]。

#### 4.5.4 MemSQL
MemSQL[2136]是一种旨在执行事务和分析的混合数据库，在体系结构上，MemSQL在普通硬件上扩展而来。引擎被组织成两层：（1）聚合节点与客户端接口并执行查询路由，解析和优化，而（2）叶子节点提供内存存储和查询处理功能。在内部，MemSQL避免了基于页面的间接访问，并使用无锁的skiplists来索引包含指向记录的直接内存指针的数据。MemSQL通过刷新仅redo的事务日志来提供持久性。与Hekaton一样，MemSQL使用日志定期创建数据库的完整快照。数据库使用最后一个有效检查点恢复，并通过重播仅redo事务日志[100]使其保持一致。
#### 4.5.5 Silo

Silo [151]是一个在Masstree上面建立的高性能主存数据库[97]。 Silo以存储过程的形式支持事务。 在架构上，Silo表包含一组在主键上索引的内存记录，以及任意数量的二级索引（可能没有）。 索引存储直接记录指针，没有基于页面的间接指令。

Silo旨在扩展到大型多核机器上。 Silo性能和可扩展性的关键在于它减少了对共享内存中热点的写入。 Silo使用基于epoch的方法，而不是为事务分配唯一的时间戳（需要对每个事务进行原子获取和添加的热点的一个示例），其中所有事务都读取全局时钟编号E，每隔一段时间递增一次（40 ms默认情况下）。 E占用每个事务id的高位，并定义事务的序列顺序。

Silo提供如下可序列化的事务。 事务跟踪它们的读取集并缓冲它们的写集。 提交时，事务获取其写集中的所有记录锁（如果不能获取则中止），通过读取当前的全局时期生成其id，验证其读取集（如果验证失败则中止），最后装载其写集及其事务ID和释放写锁。 幻像保护由版本化索引叶节点提供。 在验证期间，事务会根据索引中的当前版本检查它读取的叶子版本，如果存在差异则中止。

Silo在其耐用性和恢复设计中利用多核并行性[159]。 Silo在多个磁盘上并行执行仅redo的日志记录，确保在更新值更大的epoch之前，对epoch的更新是持久的。 Silo通过协作调度检查点工作线程并行执行数据库检查点。 通过并行重放检查点文件来重建内存中状态，然后进行并行日志重放以使数据库处于一致状态，从而并行化恢复。
## 5. 当前的研究和未来的方向
&emsp;&emsp;在之前的部分，我们研究了当前的内存数据库的结构和实现。我们调查的大部分技术，要么是在生产系统中实际应用的，要么是可以作为当前研究基础的成熟的研究原型。  
&emsp;&emsp;在这一部分，我们总结了主存数据库系统中活跃的研究领域，这些系统本质上更具投机性（speculative）。第一个内容是“冷数据管理”，这一部分研究如何处理那些很少被访问，无需放在内存中的数据。第二个内容是研究如何用事务型内存简化高度并发的主存系统的实现。然后，我们总结了非易失性（non-volatile）RAM在数据库系统中的应用：一个更具推测性（speculative）的研究领域，探索如何调节系统使其能有效使用新的高性能的，字节可寻址的存储介质。
### 5.1 冷数据处理
&emsp;&emsp;在大多数OLTP工作负载中，某些记录是“热门”并经常访问（工作集），而其他的是“冷门”的，不常被访问。内存数据库的高性能是由存储在内存中的热数据体现的。相对而言，冷数据不需要存储在内存中。如何管理“冷”数据是内存数据库系统中的一个活跃的研究领域。这项工作的目标是有效地识别并将冷记录（cold records）移出系统的关键热门路径（hot path）。这一领域的研究受两个主要观察结果的推动：
1. _OLTP中工作负载的不均衡。_ 实际的事务性工作负载通常表现出相当大的访问偏差。例如，UPS或FedEx等公司的包跟踪工作负载表现出与时间相关的偏差。一个新包的记录通常直到递交时才频繁更新，之后会有一段时间进行分析，再之后就很少被访问。另一个例子是在亚马逊等大型电子商务网站上发现的自然偏差，在这些电商网站上，受欢迎的东西被访问的频率要更高。这些偏好可能会随着时间而改变，但通常不会很快
2. _经济问题。_ 将冷数据存在二级存储器中要比存储在DRMA(动态随机存取存储器)中便宜的多。高密度服务器级DRAM的价格非常高，使得像闪存这样的媒体对冷数据很有吸引力。实际上，使用当前硬件价格计算Gray和Putzolu [56]提出的5分钟规则表明，如果至少每隔60分钟访问一条200字节的记录，它应存储在内存中。随着记录大小的增加，此时间窗口减少; 更完整的分析可以在[88]中找到。

&emsp;&emsp;在剩下的章节中，我们总结了当前的主存数据库中处理冷数据的三种技术。总的来讲，对冷数据的处理仍处于研究阶段；还没有在产品中的大规模应用。
#### HyPeR Compression
&emsp;&emsp;HyPeR的冷数据处理策略[48]将冷数据和热数据分开，以OLAP查询的读优化格式压缩它。此技术在虚拟内存页面级别执行冷/热数据分类，这是其数据组织的粒度。分类搭载在CPU内存管理单元上，设置脏页标志用于页帧重定位。Hyper将页面固定在内存中，因此它能够读取和重置脏标志以帮助对冷页面和热页面进行分类。分类后，冷记录块被压缩以减少内存消耗。冷记录块存储在巨大的虚拟内存页上，并且不可变，以便在OLAP快照中使用。

&emsp;&emsp;**Data Blocks**。HyPeR对冷数据处理的后续工作中，使用了数据块（Data Blocks）：以字节可寻址的压缩格式存储一个或多个属性块的自包含容器[75]。目标是在保持OLAP的高性能的同时节省内存。通过维护一个没有指针的扁平结构，数据块也适合用于辅助存储，从而减少了HyPeR的DRAM占用。数据块包含重建存储属性所需的所有数据和我们新的轻量级位置SMA(PSMA)索引结构，但没有元数据（如模式信息），因为在每个块中复制这些元数据将浪费空间。尽管与这项工作是正交的（不相关，orthogonal），但数据块的设计还考虑了即将推出的辅助存储解决方案，包括非易失性RAM（NVRAM）和字节可寻址闪存。存储在这种存储设备上的数据块中的数据可以被直接寻址和读取，而不需要将整个数据块放入DRAM中。
#### Hekaton Siberia
&emsp;&emsp;Hekaton的Siberia方法将数据库分为热存储和冷存储[42]。此拆分位于Hekaton的光标界面之下，向更高的软件层隐藏了记录的物理位置。Siberia采用了一种新的冷数据分类技术，它对记录访问的样本记日志，然后并行分析日志以区分热记录和冷记录[88]。Siberia在冷数据存储中使用了小的近似但保守的点和范围滤波器[10]。为了防止在处理过程中对冷存储不必要的访问。Siberia还与Hekaton的乐观多版本并发控制方案相吻合，能够执行跨越热存储和冷存储的事务，以及在数据库联机和处于活动状态时将数据迁移到冷存储和从冷存储迁移数据。
#### Anti-Caching
&emsp;&emsp;Anti-Caching[34]是一种内置在H-Store中的面向内存的DBMS设计，它允许系统管理大于可用内存量的数据库，而不会招致面向磁盘系统的性能损失。当内存中的数据量超过SAN管理员定义的阈值时，DBMS会将数据移动到磁盘以释放空间用于新数据。 DBMS为每个被逐出的元组维护一个内存中的“墓碑”。当一个正在运行的事务试图通过它的墓碑访问一个被逐出的元组时，DBMS会中止该事务，并从anti-cache中提取它需要的事务，而不阻塞其他事务。 一旦将事务所需的数据移回内存中，事务就会重新启动。   
&emsp;&emsp;在运行时，H-Store监视数据库使用的主内存量。当数据库的大小相对于节点上的可用内存量超过某个管理员定义的阈值时，DBMS将冷数据“驱逐”到anti-cache，以便为新数据腾出空间。为此，DBMS构造一个固定大小的块，其中包含数据库中最近最少使用的（LRU）元组，并将该块写入anti-cache。然后它更新一个内存驻留目录，跟踪被驱逐的每个元组。当事务访问其中一个被驱逐的元组时，HStore将该事务切换为"pre-pass"模式，以了解事务所需的元组。在此"pre-pass"完成后，H-Store随后中止该事务（回滚它可能做出的任何更改）并在系统在后台检索元组时保留它。将数据合并回内存表后，将释放并重新启动事务。    
&emsp;&emsp;anti-caching根据每个表使用LRU替换策略对冷数据进行分类。标识LRU最近的统计信息存储在内存中的元组标头中。Anti-caching使用块表来存储冷记录，其中块的设计理念与磁盘页类似。内存中的逐出表用于将逐出的元组映射到辅助存储上的相应块。Anti-caching使用特殊的单分区事务来序列化冷元组并将其迁移到块存储。触及冷记录的事务被中止并重新启动；在重新启动之前，事务进入一个pre-pass阶段，该阶段尝试标识将被访问的所有冷记录，并将这些记录迁移到内存中。一旦预传递阶段完成，事务将重新启动。
### 5.2 事务型内存（Transactional Memory）
&emsp;&emsp;Transactional Memory(TM)允许在关键部分中原子的执行任意加载和存储，从而使程序员不必考虑细粒度线程级并发性。事务内存的研究始于70年代末[94]，90年代初Herlihy和Moss提出了第一个实际的实现方案[64]。然而，使用TM来简化高性能数据库系统的实现从未站稳脚跟；这主要是由于在软件中实现的事务对性能造成了很大的影响。  
&emsp;&emsp;Intel和IBM最近发布的硬件事务性内存(HTM)使研究数据库和事务性内存的热潮死灰复燃。虽然细节有所不同，但基本思想是在现有CPU特性上使用HTM实现，例如，CPU缓存用于存储事务缓冲区并提供隔离。此外，CPU缓存一致性协议可用于检测事务冲突。所有这些都导致了一个非常低开销的事务性内存实现。  
&emsp;&emsp;虽然HTM是有效的，但它目前的形式确实有一些局限性，这些局限性已经在最近的一些工作中得到了实验验证[71，86，95]。一个主要的约束是事务的读写集必须适合缓存才能执行，因此许多属性可能限制事务的大小，包括:缓存容量、缓存集相关性、超线程性、TLB容量等。另一个限制是事务持续时间。许多硬件的问题，如中断、上下文切换或页面错误，都会中止传输。此外，冲突检测通常是在高速缓存行的粒度上进行的。这可能会导致错误共享的情况，即由于线程访问和修改同一缓存行上的不同项而导致中止。考虑到这些限制，当前一个公开的问题是:HTM如何可靠地适应高性能数据库系统？本节的其余部分总结了该领域的最新研究成果。
#### 5.2.1 使用HTM提高并发性
&emsp;&emsp;Hyper团队研究了如何利用HTM在主内存数据库系统中实现并发[86]。他们的方法通过将事务分解为单独的记录访问（读取或更新）来实现线程级并行性，其中每个访问都在硬件事务中执行。这种方法很好地利用了HTM的限制，因为单记录操作是短暂的，而且相当可预测。为了实现事务，使用时间戳排序并发控制将这些HTM访问“粘在一起”。  
&emsp;&emsp;DBX[153]是一个原型OLTP系统，用于评估HTM作为实现事务的通用解决方案。DBX使用乐观并发控制方法，跟踪其读集并缓冲其写操作。在提交之前，事务验证读取集，并在任何并发事务修改了集中的记录时中止。为了应用写操作，DBX使用一个HTM事务来安装它的更新，从而在并发执行的线程之间进行同步。与Hyper不同的是，DBX尝试在一个HTM事务中批处理所有记录更新。
#### 5.2.2 HTM and Indexing(索引)
&emsp;&emsp;Karnagel等人探讨了如何使用HTML锁省略来提高B+树和SAP Hana的增量存储索引中的线程级并发性[71]。使用锁省略进行编程与使用锁（锁存器）进行编程类似，但锁定省略首先尝试以事务方式执行关键部分，并且只有在事务中止时才会通过获取锁定来执行关键部分。锁省略的好处是它为使用简单粗粒度锁的程序提供了乐观的并发性。这项工作发现，使用具有全局省略锁的HTM可以为具有小固定大小的键值的数据库带来出色的性能，但必须在索引实现中（在delta索引的情况下）必须小心，以避免虚假的HTM因错误而中止 缓存线共享。  
&emsp;&emsp;Makreshanski等人探索了高性能无锁的B+树和基于HTM的B+树之间的交互作用，其中，基于HTM的B+树通过使用全局省略锁来实现并行性[95]。 这项工作提出了三项主要意见。首先，对于在索引中实现并发性来说，HTM是一种没有吸引力的通用解决方案，因为当索引的键值和页面过大时（由于事务大小和相关性冲突而中止），单个只读操作可以中止。此外，由于无法从事务中移除缓存行，因此无法用HTM实现细粒度并发技术，如锁抓取。其次，无锁设计具有基本的性能优势，因为它从不废弃读操作，而基于HTM的方法可以在数据冲突时废弃读操作。第三，使用HTM实现高性能的多字比较交换可以以大约10-15%的开销大大降低无锁索引设计的复杂性。  
&emsp;&emsp;最近关于DRTM的工作探索了使用HTM和RDMA[154]构建的向外扩展的事务性键值对存储方式。构建DrTM中的挑战包括避免有限的HTM事务工作集，以及避免在HTM事务内进行RDMA调用，这个调用会由于中断而导致自动中止。为了避免长事务，DRTM使用事务截断来控制事务的大小。为了避免在HTM事务内进行RDMA调用，DRTM执行两阶段锁定来“锁定”远程记录，将它们读入本地缓存，更新它们，并在提交后写回本地更改。
### 5.3 非易失性RAM
&emsp;&emsp;非易失性RAM(Non-volatile RAM，NVRAM)是指开发中的一大类技术，它们承诺融合了DRAM和硬盘的最佳属性，例如高性能，字节寻址能力，电源故障后的持久性和能效。虽然有多个实现NVRAM的方案，包括相变存储器（PCM）[128]，忆阻器[144]和STT-MRAM [40]，但供应商希望NVRAM的价格/性能（价格性能比）达到低于DRAM（在价格和性能两方面）， 以及高于Flash / SSD的目标，为NVRAM提供一个在存储层次结构中的合适的位置。   
&emsp;&emsp;虽然NVRAM还没有上市(电池支持的RAM（或NVDIMM）可被视为NVRAM的一种形式，目前已上市。 然而，NVDIMM将遵循类似于DRAM的价格/性能趋势。)，但是已经有了关于如何使数据库系统有效地使用NVRAM的推测性研究。 本节的其余部分对这项工作进行了分类和总结。
#### 5.3.1 Logging（日志）
&emsp;&emsp;NVRAM研究的一个主要领域是“传统”ARIES类型的恢复技术，探索将恢复日志放在NVRAM上。Pelley等人。探讨了一种群体提交方法（类似于Dewitt等人[37]提出的方法），它在Shore-MT [121]中批量恢复日志写入。这项工作的目标是减少确保正确排序NVRAM上的日志所需的写障碍的数量。 Wang和Johnson [152]探索了Shore-MT中的分布式日志记录协议，该协议为每个日志配备了基于NVRAM的写缓冲区。 一旦将提交记录写入NVRAM缓冲区，就会提交事务，从而避免集中的日志记录瓶颈。由于处理器缓存是易失性的，因此本文解决的一个主要问题是如何保证分布式日志记录的持久性。解决方案是使用被动组提交协议，当确保事务持久性所需的所有记录从CPU缓存刷新到NVRAM时，本协议会对这些记录进行跟踪。Fang等人，使用了一种与SolidDB Log Manager[45]探索类似的方法。这项工作解决了检测部分写入的问题以及对数据库恢复的影响。    
&emsp;&emsp; MARS [31]重新考虑ARIES风格的日志记录和恢复，以解决NVRAM的独特特性。MARS是一种新颖的预写日志记录方案，可执行仅重做日志记录。 MARS的关键特性是引入了一个称为 edible atomic write（或EAW）的多部分原子写原语。EAW是一组每个对象的重做日志项（ a set of per-object redo log
entries），可以在事务执行期间多次更新。在提交时，存储硬件将更新复制到目标位置; 如果发生故障，该副本将保证成功。
#### 5.3.2 Storage（存储）
&emsp;&emsp;另一个主要研究领域涉及针对字节寻址NVRAM优化的存储和编程模型。REWIND [29]是一个轻量级用户模式库，用于在NVRAM中实现任意持久数据结构。用户使用事务接口更新REWIND数据结构。在接口（hood）下，REWIND透明地管理更新日志并管理事务的提交和恢复。  
&emsp;&emsp;SOFORT [115]是一种存储引擎，设计用于DRAM和NVRAM的两级层次结构。引擎不执行日志记录。它使用多版本并发控制直接使用新版本更新NVRAM。SOFORT的目标是实现几乎即时恢复;由于NVRAM中的更新过程是持续的，因此无需从检查点和恢复日志“重建”数据库。为此，SOFORT智能地选择在NVRAM中实现数据库引擎的哪个部分，而不是实现完全的NVDRAM，通过这种方式来限制恢复时间的大小。  
&emsp;&emsp;Arulraj等人。[14]通过实验评估英特尔NVRAM硬件仿真器上的三种不同存储引擎。评估的存储方法包括：就地更新，写时复制和日志结构更新。此工作还针对每种方法实现了NVRAM优化的变体，以解决CPU开销，存储空间和耗损均衡问题。该评估的结果表明，NVRAM优化的就地更新存储方法在开销和设备损耗均衡方面是理想的。与SOFORT一样，本研究观察到直接更新数据库会导致崩溃后几乎立即重启。

